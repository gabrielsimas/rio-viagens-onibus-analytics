{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielsimas/rio-viagens-onibus-analytics/blob/feature%2Fcriacao-ambiente/ARTIGO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**IMPORTANTE**: ***Executar esse trecho de c√≥digo para baixar o reposit√≥rio e alimentar as imagens desse arquivo.***"
      ],
      "metadata": {
        "id": "3kv79LrcVKAx"
      },
      "id": "3kv79LrcVKAx"
    },
    {
      "cell_type": "markdown",
      "id": "af5e85ad",
      "metadata": {
        "id": "af5e85ad"
      },
      "source": [
        "# üöå MVP Engenharia de Dados - O Rio de Janeiro e os √înibus: Entendendo o Caos!\n",
        "## ***Nome:*** **Lu√≠s Gabriel Nascimento Simas**\n",
        "## ***Matr√≠cula:*** **4052025000943**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c17a22a",
      "metadata": {
        "id": "0c17a22a"
      },
      "source": [
        "## **Objetivo**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911589b2",
      "metadata": {
        "id": "911589b2"
      },
      "source": [
        "### O Rio de Janeiro, mundialmente conhecido por suas belezas naturais, enfrenta um desafio urbano de propor√ß√µes continentais: a mobilidade p√∫blica. O sistema de transporte rodovi√°rio, vital para o funcionamento da metr√≥pole, √© frequentemente associado √† palavra \"**Caos**\".\n",
        "### A popula√ß√£o carioca convive diariamente com incertezas: tempos de espera imprevis√≠veis, frota envelhecida, desconforto t√©rmico em uma cidade tropical e falhas na cobertura do servi√ßo, especialmente em hor√°rios noturnos. Embora iniciativas como o BRT (*Bus Rapid Transit*) e o BRS (*Bus Rapid System*) tenham sido implementadas para mitigar a lentid√£o, a percep√ß√£o de qualidade do servi√ßo permanece aqu√©m do ideal.\n",
        "### N√£o se trata apenas de desconforto, mas de um problema sist√™mico que envolve a gest√£o de cons√≥rcios privados, a fiscaliza√ß√£o do poder p√∫blico e o impacto direto na qualidade de vida e produtividade do cidad√£o.\n",
        "### Se por um lado o cen√°rio f√≠sico √© ca√≥tico, o cen√°rio digital oferece uma oportunidade de organiza√ß√£o e entendimento. A Prefeitura da Cidade do Rio de Janeiro (PCRJ), atrav√©s do portal *[data.rio](https://data.rio)*, disponibiliza um vasto reposit√≥rio de dados abertos em v√°rios formatos e alguns at√© como tabelas no **Google** *BigQuery*.\n",
        "### Esses dados incluem registros detalhados de viagens (baseados em bilhetagem e telemetria consolidada), cadastro t√©cnico da frota, dados meteorol√≥gicos do Alerta Rio e registros de reclama√ß√µes da central 1746."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3896e5c3",
      "metadata": {
        "id": "3896e5c3"
      },
      "source": [
        "### Para guiar o desenvolvimento do pipeline de dados, definimos as seguintes quest√µes-chave que esse trabalho deseja responder."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2672dc5",
      "metadata": {
        "id": "d2672dc5"
      },
      "source": [
        "\n",
        "- ### **Din√¢mica de Viagens e Demanda**\n",
        "    - #### 1. Em quais dias e faixas hor√°rias o volume de viagens realizadas atinge seu pico?\n",
        "    - #### 2. Existe uma redu√ß√£o sens√≠vel na oferta de viagens nos finais de semana comparados aos dias √∫teis?\n",
        "    - #### 3. Qual √© a dura√ß√£o m√©dia das viagens por Cons√≥rcio? Existe discrep√¢ncia significativa entre eles?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42967d32",
      "metadata": {
        "id": "42967d32"
      },
      "source": [
        "- ### **Consist√™ncia Operacional**\n",
        "    - ### 4. O tempo de viagem nas principais linhas √© consistente ou apresenta alto desvio padr√£o (imprevisibilidade)?\n",
        "    - ### 5. Em dias classificados como \"Chuvosos\" (precipita√ß√£o m√©dia acima de um limiar), observa-se aumento no tempo m√©dio de deslocamento?\n",
        "    - ### 6. A chuva influencia na quantidade total de viagens realizadas (redu√ß√£o da oferta ou demanda)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be726d7",
      "metadata": {
        "id": "5be726d7"
      },
      "source": [
        "### **Escopo temporal**\n",
        "### A an√°lise compreender√° o per√≠odo de **01/01/2024** a **19/12/2025**, permitindo uma vis√£o longitudinal que abrange sazonalidades, dias √∫teis, feriados e varia√ß√µes clim√°ticas ao longo de quase dois anos completos.\n",
        "### Antes de come√ßar a explicar o mecanismo de coleta de dados que foi utilizado, √© importante mostrar em qual arquitetura nosso trabalho est√° sendo desenvolvido/implementado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af9bd61",
      "metadata": {
        "id": "2af9bd61"
      },
      "source": [
        "## üèõÔ∏è **Arquitetura da Solu√ß√£o**: Data Lakehouse H√≠brido (VPS + GCP)\n",
        "\n",
        "### Esta se√ß√£o detalha os componentes tecnol√≥gicos do MVP. A arquitetura adota uma estrat√©gia **H√≠brida**, combinando a otimiza√ß√£o de recursos de um VPS (Virtual Private Server) com a escalabilidade de armazenamento da Nuvem P√∫blica (Google Cloud).\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Provisionamento de Infraestrutura (IaC)**\n",
        "\n",
        "### A infraestrutura na nuvem n√£o foi criada manualmente. Todo o ambiente (Buckets, Permiss√µes, APIs) foi definido via **Terraform**, garantindo que o ambiente seja reprodut√≠vel, audit√°vel e imune a erros humanos.\n",
        "\n",
        "### üèóÔ∏è **Terraform Workflow**\n",
        "### **Papel:** Provisionamento de Nuvem.\n",
        "* ### **Fun√ß√£o:** Cria e gerencia os recursos na GCP (Landing, Bronze, Silver, Gold) de forma declarativa.\n",
        "* ### **Ciclo de Vida:** O deploy seguiu as fases de *Bootstrap*, *Validation*, *Plan* e *Apply*.\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>\n",
        "<strong>\n",
        "Clique para expandir: Evid√™ncias da Execu√ß√£o do Terraform\n",
        "</strong>\n",
        "</summary>\n",
        "<br>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">1. Prepara√ß√£o do Ambiente (Bootstrap)</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1Iq7M2xjMHbjlrEMHYy36VCb8tu1WXJQZ\" width=\"100%\" alt=\"Script Inicial\">\n",
        "<p><em>Setup inicial das vari√°veis e autentica√ß√£o no Google Cloud Shell.</em></p>\n",
        "<hr>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">2. Valida√ß√£o e Planejamento (Plan)</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1tdGPpca2xtP4oKW0zmfELg5opni451Ao\" width=\"100%\" alt=\"Terraform Validate\">\n",
        "<p><em>Validando a sintaxe dos arquivos .tf para garantir qualidade do c√≥digo.</em></p>\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1PIAl4zRuyEZYhEJJVuPNaItXQIO99RUm\" width=\"100%\" alt=\"Terraform Plan\">\n",
        "<p><em>Terraform Plan: O sistema calcula o \"estado futuro\" e prev√™ as mudan√ßas antes de tocar na nuvem.</em></p>\n",
        "<hr>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">3. Aplica√ß√£o e Resultado (Apply)</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1faj6oTVGA2jjHIyjzZ5EzMl1kRyUxE8S\" width=\"100%\" alt=\"Terraform Apply Sucesso\">\n",
        "<p><em>Terraform Apply: Cria√ß√£o automatizada dos recursos (Buckets, Service Accounts).</em></p>\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1JaqcI2S5XlZiu6WKHt6Ibn3nzoiGHKzK\">\n",
        "<p><em>Resultado Final: O ambiente pronto no console da Google Cloud, refletindo exatamente o c√≥digo.</em></p>\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Servidor de Processamento (Compute On-Premise)**\n",
        "\n",
        "###Para viabilizar o uso de ferramentas corporativas (Dremio/Java) em um hardware limitado, foi realizado um trabalho avan√ßado de **Engenharia de Sistemas** no VPS.\n",
        "\n",
        "### üñ•Ô∏è **Especifica√ß√µes e Otimiza√ß√£o**\n",
        "* ### **Hardware:** VPS com processador AMD EPYC (4 vCPUs) e **6GB de RAM**.\n",
        "* ### **Estrat√©gia de Swap:** Devido √† limita√ß√£o de mem√≥ria f√≠sica, configurou-se uma √°rea de swap de **22GB+** via terminal Linux. Isso permite que o *Heap* do Java (Dremio) e os processos do Airflow rodem simultaneamente sem causar *Out Of Memory (OOM)*.\n",
        "\n",
        "### üêß **Hardening e Seguran√ßa**\n",
        "### O ambiente segue o princ√≠pio do menor privil√©gio, evitando a execu√ß√£o de containers como `root` e utilizando chaves SSH para acesso.\n",
        "\n",
        "<details>\n",
        "<summary><strong>üé• Clique para expandir: Engenharia de Sistemas (Linux Tuning)</strong></summary>\n",
        "<br>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">1. Acesso Seguro e Cria√ß√£o de Usu√°rio</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1KVESwnVCpXmFbDKCJSFuEQzm7-cp6eB4\" width=\"100%\" alt=\"Acesso SSH\">\n",
        "<p><em>Acesso remoto seguro via SSH com chaves criptografadas (sem senha).</em></p>\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1-ruL8NLRbhD9FZ333c7H0A2sVInfK411\" width=\"100%\" alt=\"User Setup\">\n",
        "<p><em>Seguran√ßa: Cria√ß√£o de usu√°rio dedicado para evitar execu√ß√£o de containers como root.</em></p>\n",
        "<hr>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">2. Expans√£o de Mem√≥ria (Swap Strategy)</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1jy_hFiKoHqKumGM-0ZMaYvaQ8GZdCEdN\" width=\"100%\" alt=\"Swap Creation\">\n",
        "<p><em>Engenharia: Cria√ß√£o manual de arquivo de Swap e ajuste de swappiness para suportar carga pesada.</em></p>\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Orquestra√ß√£o e Ingest√£o**\n",
        "\n",
        "### üê≥ **Docker Compose**\n",
        "### **Papel:** Orquestra√ß√£o de Containers.\n",
        "* ### **Fun√ß√£o:** Sobe e conecta os servi√ßos do Airflow, Dremio, Nessie e Postgres em uma rede interna isolada, garantindo portabilidade.\n",
        "\n",
        "Aqui, temos uma imagem de nosso ambiente com os containers utilizados nessa solu√ß√£o operacionais, prints das aplica√ß√µes Lazydocker e Portainer respectivamente.\n",
        "\n",
        "![Containers em p√© com LazyDocker](https://drive.google.com/uc?id=1tQUFttV93jUKBNndXEnMsOfUpfpn0s_Q)\n",
        "\n",
        "![Container em p√© com Portainer](https://drive.google.com/uc?id=14HgdV4boaHVyKzcbDsesDmg0peh_aF2J)\n",
        "\n",
        "\n",
        "### üå™Ô∏è **Apache Airflow**\n",
        "### **Papel:** Orquestrador Geral (O \"Maestro\").\n",
        "* ### **Fun√ß√£o:** Gerencia todo o pipeline de dados. Monitora o Google Drive, dispara os workers de ingest√£o e aciona o **dbt** para as transforma√ß√µes.\n",
        "* ### **Otimiza√ß√£o:** Configurado com `LocalExecutor` para paralelizar tarefas nos 4 n√∫cleos da CPU.\n",
        "\n",
        "![Tela do Airflow com DAGs](https://drive.google.com/uc?id=1AYCCyarwBsubUZ0F-i33rFUd5FrVgIHD)\n",
        "\n",
        "### ü¶Ü **DuckDB (Worker)**\n",
        "### **Papel:** Processamento Leve de Ingest√£o.\n",
        "* ### **Fun√ß√£o:** Banco anal√≠tico embutido (*in-process*) que roda dentro do worker do Airflow.\n",
        "* ### **No MVP:** L√™ arquivos CSV brutos da camada Landing, limpa e converte para **Parquet** na camada Bronze. Sua efici√™ncia permite processar gigabytes de dados com pouca RAM.\n",
        "\n",
        "### ‚òÅÔ∏è **Google Cloud Storage (GCS)**\n",
        "### **Papel:** Armazenamento de Objetos (Storage Layer).\n",
        "* ### **Camadas:**\n",
        "    * ### **Landing:** Dados brutos (CSV).\n",
        "    * ### **Bronze:** Dados hist√≥ricos convertidos para Parquet.\n",
        "    * ### **Silver/Gold:** Dados de neg√≥cio no formato **Apache Iceberg**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **O Motor do Lakehouse (Engine & Transforma√ß√£o)**\n",
        "\n",
        "### üíé **Dremio**\n",
        "### **Papel:** SQL Lakehouse Engine.\n",
        "* ### **Fun√ß√£o:** Motor de consulta que permite executar SQL diretamente sobre o Data Lake.\n",
        "* ### **Performance:** Tunado com **Heap de 2.5GB** e **Direct Memory de 1GB** para operar no limite do hardware, delegando carga excedente para o Swap.\n",
        "\n",
        "### üßä **Apache Iceberg**\n",
        "### **Papel:** Formato de Tabela Aberto.\n",
        "* ### **Fun√ß√£o:** Camada de abstra√ß√£o que traz transa√ß√µes ACID e *Time Travel* para o Data Lake, permitindo updates e deletes seguros em arquivos Parquet.\n",
        "\n",
        "### üõ†Ô∏è **dbt (data build tool)**\n",
        "### **Papel:** L√≥gica de Transforma√ß√£o.\n",
        "* ### **Fun√ß√£o:** Compila as regras de neg√≥cio (SQL) e envia para o Dremio executar. Transforma dados da Bronze em tabelas Silver (limpas) e Gold (agregadas).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Governan√ßa**\n",
        "\n",
        "### ü¶ï **Project Nessie**\n",
        "### **Papel:** Cat√°logo de Dados (Git-for-Data).\n",
        "* ### **Fun√ß√£o:** Gerencia as vers√µes das tabelas Iceberg.\n",
        "* ### **Recurso:** Permite criar *Branches* de dados para testes isolados antes de fazer *Merge* na produ√ß√£o, similar ao Git para c√≥digo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3b6231",
      "metadata": {
        "id": "7b3b6231"
      },
      "source": [
        "### **O que √© a Arquitetura Moderna de Dados** ?\n",
        "### √â uma arquitetura **flex√≠vel**, **escal√°vel** e **baseada na nuvem**. Ela surge dos modelos tradicionais para suportar o volume e a velocidade dos dados atuais, integrando **Big Data**, **IA** e **governan√ßa**. Ela tem como caracter√≠sticas principais:\n",
        "- ### **Nuvem** e **Tempo Real**: Prioriza a elasticidade (AWS, Azure, GCP) e a disponibilidade imediata dos dados para tomadas de decis√£o r√°pidas. Neste MVP, vamos utilizar a GCP como nuvem principal, mas a solu√ß√£o que vamos implementar √© totalmente agn√≥stica em quest√µes de hospedagem.\n",
        "- ### **Dados como Produto**: O gerenciamento √© focado no valor que o dado gera para o neg√≥cio.\n",
        "- ### **Governan√ßa e Seguran√ßa**:  Implementadas na base (considerando LGPD), com forte gest√£o de metadados e interoperabilidade via APIs.\n",
        "### As abordagens mais populares dessa arquitetura s√£o:\n",
        "- ### **Data Lakehouse**: O melhor dos dois mundos (flexibilidade do Lake + confiabilidade do Warehouse). ***Ser√° a arquitetura que utilizaremos nesse MVP.***\n",
        "- ### **Data Mesh**: Descentraliza a propriedade dos dados por dom√≠nios de neg√≥cio.\n",
        "- ### **Modern Data Stack (MDS)**: Orquestra√ß√£o de diversas ferramentas especializadas.\n",
        "\n",
        "### Tem como principal objetivo unificar dados, an√°lises e cargas de trabalho de Intelig√™ncia Artificial de forma **eficiente, confi√°vel e escal√°vel**.\n",
        "### ***Neste MVP, vamos focar no uso do Data Lakehouse com a Arquitetura Medalh√£o***\n",
        "### Antes da se√ß√£o sobre a infraestrutura que ser√° utilizada, passaremos rapidamente sobre os significados e prop√≥sitos do **Data Lakehouse** e **Arquitetura Medalh√£o**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ca2540",
      "metadata": {
        "id": "e8ca2540"
      },
      "source": [
        "### **Data Lakehouse**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1jV69fy2AMAd4YZx0BACaQPwHQLkCJNH1\" alt=\"Girl in a jacket\" width=\"400\">\n",
        "<center />"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O **Data Lakehouse** representa uma evolu√ß√£o significativa na arquitetura de dados, surgindo para resolver a dicotomia hist√≥rica entre Data Warehouses e Data Lakes.\n",
        "### A premissa b√°sica √© simples, mas poderosa: **implementar funcionalidades de gest√£o de dados (t√≠picas de Warehouses) diretamente sobre o armazenamento de baixo custo e flex√≠vel (t√≠pico de Lakes)**.\n"
      ],
      "metadata": {
        "id": "HpUTZOVTZtl7"
      },
      "id": "HpUTZOVTZtl7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **O Problema Original (Silos de Dados)**\n",
        "Para entender o Lakehouse, precisamos olhar para o que existia antes:\n",
        "- ### **Data Warehouse (DW)**: √ìtimo para dados estruturados, SQL r√°pido e confiabilidade (ACID). **Entretanto**, √© caro, r√≠gido e p√©ssimo para lidar com dados n√£o estruturados (v√≠deo, √°udio, logs brutos) ou ML (Machine Learning).\n",
        "- ### **Data Lake**: √ìtimo para armazenamento (arquivos brutos) e flex√≠vel para Ci√™ncia de Dados. **Entretanto**, tende a virar um \"Data Swamp\" (p√¢ntano de dados) sem governan√ßa de Dados, n√£o suporta transa√ß√µes complexas e tem performance pobre para BI tradicional.\n",
        "\n",
        "### Muitas empresas mantinham os dois sistemas duplicados, criando complexidade e alto custo de manuten√ß√£o."
      ],
      "metadata": {
        "id": "SJn0qKNBZ0rj"
      },
      "id": "SJn0qKNBZ0rj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **O Conceito do Lakehouse**\n",
        "### O Data Lakehouse elimina a necessidade de mover os dados do Lake para o Warehouse para que sejam √∫teis. Ele adiciona uma camada de metadados e governan√ßa sobre os arquivos brutos armazenados no Lake."
      ],
      "metadata": {
        "id": "991cstBSalHS"
      },
      "id": "991cstBSalHS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Principais Pilares T√©cnicos:**\n",
        "- ### **Transa√ß√µes ACID**: Esta √© a grande virada de chave. O Lakehouse permite que m√∫ltiplos usu√°rios leiam e escrevam dados simultaneamente sem corromper a base. Se uma carga de dados falha no meio, nada √© salvo (atomicidade), garantindo que os dados estejam sempre consistentes.\n",
        "- ### **Schema Enforcement (Imposi√ß√£o de Esquema)**: O sistema previne que dados \"sujos\" ou fora do padr√£o sejam inseridos nas tabelas, garantindo a qualidade necess√°ria para relat√≥rios de BI.\n",
        "- ### **Formatos Abertos**: Diferente dos DWs tradicionais que usam formatos propriet√°rios (fechados), o Lakehouse armazena dados em formatos open-source como **Parquet** ou **ORC**. Isso evita o *vendor lock-in* (ficar preso a um fornecedor).\n",
        "- ### **Suporte Unificado (BI + IA)**: Cientistas de dados podem acessar os arquivos diretamente para treinar modelos de IA, enquanto Analistas de BI podem rodar queries SQL na mesma fonte de dados, sem duplica√ß√£o."
      ],
      "metadata": {
        "id": "Spk9o_GHaud5"
      },
      "id": "Spk9o_GHaud5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A Camada \"M√°gica\": ***Table Formats*****\n",
        "\n",
        "### Como voc√™ transforma um monte de arquivos soltos num bucket  qualquer (nuvem) em uma tabela confi√°vel com suporte a transa√ß√µes? **Atrav√©s dos Table Formats** (Formatos de Tabela).\n",
        "\n",
        "### Existem tr√™s tecnologias principais que habilitam o Lakehouse hoje:\n",
        "\n",
        "1. ### **Delta Lake** (Criado pela **Databricks**, agora *Linux Foundation*).\n",
        "2. ### **Apache Iceberg** (Criado pela **Netflix**) (**Que ser√° utilizado neste MVP nas camadas Prata e Ouro**).\n",
        "3. ### **Apache Hudi** (Criado pela Uber).\n",
        "\n",
        "### Essas tecnologias criam uma camada de log de transa√ß√µes. Quando voc√™ faz um UPDATE ou DELETE, o sistema n√£o altera o arquivo original imediatamente; ele registra a mudan√ßa no log e gerencia vers√µes dos arquivos, permitindo at√© mesmo \"viajar no tempo\" (Time Travel) para ver como os dados estavam ontem."
      ],
      "metadata": {
        "id": "m9YoSXxsbgb2"
      },
      "id": "m9YoSXxsbgb2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Arquitetura Medalh√£o** (**Medallion Architecture**)\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1tgFGNhwxYVR6LeTWLzl9pz7mObe8UW6B\" alt=\"Arquitetura Medalh√£o\" width=\"700\">\n",
        "<center />"
      ],
      "metadata": {
        "id": "-_-WRu5_cvST"
      },
      "id": "-_-WRu5_cvST"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### √â muito comum organizar os dados dentro de um Lakehouse em tr√™s camadas l√≥gicas para garantir a qualidade:\n",
        "\n",
        "1.\t### **Bronze (Raw)**: Dados brutos, exatamente como chegaram da fonte. Nenhuma limpeza √© feita aqui.\n",
        "2.\t### **Silver (Refined)**: Dados limpos, filtrados e com tipos corrigidos. √â a \"verdade √∫nica\" da organiza√ß√£o.\n",
        "3.\t### **Gold (Curated)**: Dados agregados, modelados (ex: Star Schema) e prontos para consumo em dashboards e relat√≥rios executivos.\n",
        "\n",
        "### Al√©m disso, n√£o vamos utilizar uma stack propriet√°ria, vamos utilizar um conceito bem atual chamado **Modern Data Stack in a Box**, onde mostraremos quais ferramentas vamos usar em nossa solu√ß√£o."
      ],
      "metadata": {
        "id": "hG6_BxncdxtU"
      },
      "id": "hG6_BxncdxtU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **O Conceito: \"Modern Data Stack in a Box\"**\n",
        "### Refere-se √† implanta√ß√£o de um ambiente completo de an√°lise de dados em uma √∫nica m√°quina (como um laptop ou servidor robusto). Essa abordagem utiliza ferramentas open-source eficientes, eliminando a complexidade e o custo de Data Warehouses em nuvem para muitos casos de uso.\n",
        "### **Componentes Principais:**\n",
        "### A arquitetura replica as camadas de uma stack moderna tradicional, mas otimizada para execu√ß√£o local:\n",
        "- ### **Ingest√£o (EL)**: Ferramentas como *Airflow* ou *Airbyte* para extrair e carregar dados.\n",
        "- ### **Armazenamento (OLAP)**: Uso de bancos de dados anal√≠ticos embutidos de alta performance, sendo o *DuckDB* o principal protagonista.\n",
        "- ### **Transforma√ß√£o (T)**: Uso do *dbt* para limpeza e modelagem via *SQL* com controle de vers√£o.\n",
        "- ### **BI & Analytics**: Visualiza√ß√£o com *Apache Superset*, *Metabase* ou *Streamlit*.\n",
        "\n",
        "### **Benef√≠cios**\n",
        "- ### **Efici√™ncia de Custo**: Elimina gastos recorrentes com infraestrutura de nuvem.\n",
        "- ### **Produtividade**: Simplifica o desenvolvimento e permite Provas de Conceito (PoCs) r√°pidas sem configurar sistemas distribu√≠dos.\n",
        "- ### **Performance**: Tira proveito do hardware moderno local para processamento r√°pido.\n"
      ],
      "metadata": {
        "id": "q8DSZh3IeMxQ"
      },
      "id": "q8DSZh3IeMxQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Trade-off** (**O contraponto**)\n",
        "### Essa abordagem oferece controle total e portabilidade. **Entretanto**, a principal concess√£o √© a perda da \"escalabilidade infinita\" das plataformas de nuvem (como *Snowflake* ou *BigQuery*), j√° que o sistema est√° limitado ao hardware da m√°quina √∫nica, o que pode ser um gargalo para volumes massivos de dados ou alta concorr√™ncia de usu√°rios. Para o nosso trabalho, a estrutura que temos vai funcionar muito bem, e na se√ß√£o Conclus√£o em ***‚ÄúO que ser√° feito nessa solu√ß√£o para um futuro escal√°vel e incerto?‚Äù*** explicaremos como podemos melhorar essa infraestrutura para escala e otimiza√ß√£o futura quando os dados crescerem demais."
      ],
      "metadata": {
        "id": "IqEPHa6pfzoC"
      },
      "id": "IqEPHa6pfzoC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sobre o uso da Arquitetura neste MVP**\n",
        "### A arquitetura segue o paradigma de *Data Lakehouse*, unificando a flexibilidade de armazenamento de baixo custo (*Google Cloud Storage*) com a performance de consulta SQL (*Dremio*), sem a necessidade de um Data Warehouse propriet√°rio caro.\n",
        "### A solu√ß√£o foi desenhada para ser *agn√≥stica de nuvem* na camada de computa√ß√£o, rodando inteiramente em *cont√™ineres Docker*, o que garante portabilidade e reprodutibilidade.\"\n",
        "### A arquitetura utilizada nesse MVP eleva o n√≠vel de *\"Modern Data Stack\"* implementando um padr√£o avan√ßad√≠ssimo conhecido como **WAP** (*Write-Audit-Publish* ou *Escrever-Auditar-Publicar*), trazendo a sem√¢ntica do *Git* (*branches*, *commits*, *merges*) para a engenharia de dados.\n",
        "### O uso do *Project Nessie* como cat√°logo para gerenciar tabelas *Iceberg* √© o grande diferencial aqui. Isso resolve um dos maiores pesadelos da engenharia de dados: a inconsist√™ncia de dados durante o processamento.\n",
        "\n",
        "### Chega de conversa, vamos pra parte pr√°tica: nosso MVP funcionando propriamente dito."
      ],
      "metadata": {
        "id": "2xbyARcngNq3"
      },
      "id": "2xbyARcngNq3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=16Tbkf1vEDtZJaxeBMrT7U6XAQv7cykey\" alt=\"Talk is cheap, show me the code\" width=\"500\">\n",
        "<center />"
      ],
      "metadata": {
        "id": "Cb55z8_iMEjF"
      },
      "id": "Cb55z8_iMEjF"
    },
    {
      "cell_type": "markdown",
      "id": "e2289d76",
      "metadata": {
        "id": "e2289d76"
      },
      "source": [
        "## **Base de Dados alvo**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Como dissemos anteriormente, n√£o utilizamos apenas uma base de dados, mas multiplas de dentro do site dados.rio atrav√©s do Google BigQuery. Abaixo, mostramos o cat√°logo de metadados baseado nas informa√ß√µes do pr√≥prio provedor dos dados: A Prefeitura da Cidade do Rio de Janeiro.\n",
        "### **Disclaimer**: ***Infelizmente o tamanho da fonte nas tabelas ficou pequeno por limita√ß√µes do pr√≥prio Colab.***"
      ],
      "metadata": {
        "id": "OGFOtqikCdwj"
      },
      "id": "OGFOtqikCdwj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cat√°logo de Dados**\n"
      ],
      "metadata": {
        "id": "jVhSAY-tOCMV"
      },
      "id": "jVhSAY-tOCMV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Dataset**: *reclamacoes_1746*\n",
        "### **Tabela**: *chamados*\n",
        "### **Descri√ß√£o**: Essa tabela det√©m todos os dados de reclama√ß√µes da Central 1746 da Cidade do Rio de Janeiro, mas os dados dessa tabela s√£o apenas sobre transportes de uma maneira geral. Aqui n√£o conseguimos separar por empresa, cons√≥rcio ou dados que identifiquem o alvo das reclama√ß√µes.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **id_unidade_organizacional** | STRING | NULLABLE | Identificador √∫nico, no banco de dados, do √≥rg√£o que executa o chamado. Por exemplo: identificador da COMLURB quando o chamado √© relativo a limpeza urbana. |\n",
        "| **nome_unidade_organizacional** | STRING | NULLABLE | Nome do √≥rg√£o que executa a demanda. Por exemplo: COMLURB quando a demanda √© relativa a limpeza urbana. |\n",
        "| **id_unidade_organizacional_mae** | STRING | NULLABLE | ID da unidade organizacional m√£e do org√£o que executa a demanda. Por exemplo: \"CVA - Coordena√ß√£o de Vigil√¢ncia de Alimentos\" √© quem executa a demanda e obede a unidade organizacional m√£e \"IVISA-RIO - Instituto Municipal de Vigil√¢ncia Sanit√°ria, de Zoonoses e de Inspe√ß√£o Agropecu√°ria\". A coluna se refere ao ID deste √∫ltimo. |\n",
        "| **unidade_organizacional_ouvidoria** | STRING | NULLABLE | Booleano indicando se o chamado do cidad√£o foi feita Ouvidoria ou n√£o. 1 caso sim, 0 caso n√£o. |\n",
        "| **categoria** | STRING | NULLABLE | Categoria do chamado. Exemplo: Servi√ßo, informa√ß√£o, sugest√£o, elogio, reclama√ß√£o, cr√≠tica. |\n",
        "| **id_tipo** | STRING | NULLABLE | Identificador √∫nico, no banco de dados, do tipo do chamado. Ex: Ilumina√ß√£o p√∫blica. |\n",
        "| **tipo** | STRING | NULLABLE | Nome do tipo do chamado. Ex: Ilumina√ß√£o p√∫blica. |\n",
        "| **id_subtipo** | STRING | NULLABLE | Identificador √∫nico, no banco de dados, do subtipo do chamado. Ex: Reparo de l√¢mpada apagada. |\n",
        "| **subtipo** | STRING | NULLABLE | Nome do subtipo do chamado. Ex: Reparo de l√¢mpada apagada. |\n",
        "| **status** | STRING | NULLABLE | Status do chamado. Ex. Fechado com solu√ß√£o, aberto em andamento, pendente etc. |\n",
        "| **longitude** | FLOAT | NULLABLE | Longitude do lugar do evento que motivou o chamado. |\n",
        "| **latitude** | FLOAT | NULLABLE | Latitude do lugar do evento que motivou o chamado. |\n",
        "| **data_alvo_finalizacao** | DATETIME | NULLABLE | Data prevista para o atendimento do chamado. Caso prazo_tipo seja D fica em branco at√© o diagn√≥stico ser feito. |\n",
        "| **data_alvo_diagnostico** | DATETIME | NULLABLE | Data prevista para fazer o diagn√≥stico do servi√ßo. Caso prazo_tipo seja F esta data fica em branco. |\n",
        "| **data_real_diagnostico** | DATETIME | NULLABLE | Data em que foi feito o diagn√≥stico do servi√ßo. Caso prazo_tipo seja F esta data fica em branco. |\n",
        "| **tempo_prazo** | INTEGER | NULLABLE | Prazo para o servi√ßo ser feito. Em dias ou horas ap√≥s a abertura do chamado. Caso haja diagn√≥stico o prazo conta ap√≥s se fazer o diagn√≥stico. |\n",
        "| **prazo_unidade** | STRING | NULLABLE | Unidade de tempo utilizada no prazo. Dias ou horas. D ou H. |\n",
        "| **prazo_tipo** | STRING | NULLABLE | Diagn√≥stico ou finaliza√ß√£o. D ou F. Indica se a chamada precisa de diagn√≥stico ou n√£o. Alguns servi√ßos precisam de avalia√ß√£o para serem feitos, neste caso √© feito o diagn√≥stico. Por exemplo, pode de √°rvore. H√° a necessidade de um engenheiro ambiental verificar a necessidade da poda ou n√£o. |\n",
        "| **dentro_prazo** | STRING | NULLABLE | Indica se a data alvo de finaliza√ß√£o do chamado ainda est√° dentro do prazo estipulado. |\n",
        "| **situacao** | STRING | NULLABLE | Identifica se o chamado foi encerrado. |\n",
        "| **tipo_situacao** | STRING | NULLABLE | Indica o status atual do chamado entre as categorias Atendido, Atendido parcialmente, N√£o atendido, N√£o constatado e Andamento. |\n",
        "| **justificativa_status** | STRING | NULLABLE | Justificativa que os √≥rg√£os usam ao definir o status. Exemplo: SEM POSSIBILIDADE DE ATENDIMENTO - justificativa: Fora de √°rea de atua√ß√£o do municipio. |\n",
        "| **reclamacoes** | INTEGER | NULLABLE | Quantidade de reclama√ß√µes. |\n",
        "| **extracted_at** | TIMESTAMP | NULLABLE | Data e hora em que o registro foi extra√≠do pelo Airbyte. |\n",
        "| **updated_at** | STRING | NULLABLE | Data da ultima atualiza√ß√£o. |\n",
        "| **data_particao** | DATE | NULLABLE | Data de parti√ß√£o dos dados. Trunc(data_inicio). |"
      ],
      "metadata": {
        "id": "VuRDi6OvQX6Q"
      },
      "id": "VuRDi6OvQX6Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**: *clima_pluviometro*\n",
        "### **Tabela**: *estacoes_alertario*\n",
        "### **Descri√ß√£o**: Dados das unidades meteorol√≥gicas com seus respectivos dados, serve como dado mestre para a tabela de *taxa_precipitacao_alertario*.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **id_estacao** | STRING | NULLABLE | Identificador √∫nico da esta√ß√£o (meteorol√≥gica, pluviom√©trica ou de monitoramento) no banco de dados. |\n",
        "| **estacao** | STRING | NULLABLE | Nome ou designa√ß√£o comum da esta√ß√£o. Exemplo: \"Copacabana\", \"Tijuca\", \"S√£o Crist√≥v√£o\". |\n",
        "| **latitude** | FLOAT | NULLABLE | Coordenada geogr√°fica (latitude) da localiza√ß√£o da esta√ß√£o em graus decimais. |\n",
        "| **longitude** | FLOAT | NULLABLE | Coordenada geogr√°fica (longitude) da localiza√ß√£o da esta√ß√£o em graus decimais. |\n",
        "| **cota** | FLOAT | NULLABLE | Altitude da esta√ß√£o em rela√ß√£o ao n√≠vel do mar (geralmente expressa em metros). |\n",
        "| **x** | FLOAT | NULLABLE | Coordenada cartesiana X no sistema de proje√ß√£o (provavelmente UTM) para georreferenciamento plano. |\n",
        "| **y** | FLOAT | NULLABLE | Coordenada cartesiana Y no sistema de proje√ß√£o (provavelmente UTM) para georreferenciamento plano. |\n",
        "| **endereco** | STRING | NULLABLE | Endere√ßo f√≠sico, logradouro ou ponto de refer√™ncia onde a esta√ß√£o est√° instalada. |\n",
        "| **situacao** | STRING | NULLABLE | Status operacional atual da esta√ß√£o. Exemplo: \"Operante\", \"Nao operante\". |\n",
        "| **data_inicio_operacao** | DATETIME | NULLABLE | Data e hora em que a esta√ß√£o come√ßou a operar e coletar dados. |\n",
        "| **data_fim_operacao** | DATETIME | NULLABLE | Data e hora em que a esta√ß√£o encerrou suas atividades (caso tenha sido desativada). |\n",
        "| **data_atualizacao** | DATETIME | NULLABLE | Data e hora da √∫ltima atualiza√ß√£o cadastral ou sincroniza√ß√£o dos dados da esta√ß√£o. |\n"
      ],
      "metadata": {
        "id": "qWjDrbbVQeMF"
      },
      "id": "qWjDrbbVQeMF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**: *clima_pluviometria*\n",
        "\n",
        "### **Tabela**: *taxa_precipitacao_alertario*\n",
        "### **Descri√ß√£o**: Informa√ß√µes sobre o tempo na Cidade do Rio de Janeiro com informa√ß√µes de acumulados de chuva de 15 minutos, 1 hora, 4 horas, 24 horas e 96 horas.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **primary_key** | STRING | NULLABLE | Chave prim√°ria criada a partir da coluna id_estacao e da data_medicao. Serve para evitar dados duplicados. |\n",
        "| **id_estacao** | STRING | NULLABLE | Identificador √∫nico da esta√ß√£o de monitoramento que registrou a medi√ß√£o. (Chave estrangeira para a tabela de esta√ß√µes). |\n",
        "| **acumulado_chuva_15_min** | FLOAT | NULLABLE | Acumulado de chuva em 15 minutos. |\n",
        "| **acumulado_chuva_1_h** | FLOAT | NULLABLE | Acumulado de chuva em 1 hora. |\n",
        "| **acumulado_chuva_4_h** | FLOAT | NULLABLE | Acumulado de chuva em 4 horas. |\n",
        "| **acumulado_chuva_24_h** | FLOAT | NULLABLE | Acumulado de chuva em 24 horas. |\n",
        "| **acumulado_chuva_96_h** | FLOAT | NULLABLE | Acumulado de chuva em 96 horas. |\n",
        "| **horario** | TIME | NULLABLE | Hor√°rio no qual foi realizada a medi√ß√£o. |\n",
        "| **data_particao** | DATE | NULLABLE | Data em que foi realizada a medi√ß√£o. |\n"
      ],
      "metadata": {
        "id": "Cs7oL8LpR4uT"
      },
      "id": "Cs7oL8LpR4uT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**: *transporte_rodoviario_municipal*\n",
        "### **Tabela**: *licenciamento_frota*\n",
        "### **Descri√ß√£o**: Tabela sobre o cadastro da frota de √înibus da Cidade do Rio de Janeiro. Aqui podemos verificar a idade da frota e podemos cruzar com a tabela *viagem_onibus* onde podemos obter informa√ß√µes sobre o numero de ordem, cons√≥rcio, placa e etc.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **carroceria** | STRING | NULLABLE | Descri√ß√£o do modelo da carroceria. |\n",
        "| **id_chassi** | INTEGER | NULLABLE | C√≥digo do modelo do chassi. |\n",
        "| **id_fabricante_chassi** | INTEGER | NULLABLE | Identificador do fabricante do chassi. |\n",
        "| **nome_chassi** | STRING | NULLABLE | Descri√ß√£o do modelo do chassi. |\n",
        "| **id_planta** | INTEGER | NULLABLE | C√≥digo da planta do ve√≠culo. |\n",
        "| **tipo_veiculo** | STRING | NULLABLE | Tipo de ve√≠culo. |\n",
        "| **status** | STRING | NULLABLE | Status do ve√≠culo. |\n",
        "| **data_inicio_vinculo** | DATE | NULLABLE | Data de in√≠cio do v√≠nculo do ve√≠culo no STU. |\n",
        "| **data_ultima_vistoria** | DATE | NULLABLE | Data da √∫ltima vistoria do ve√≠culo. |\n",
        "| **ano_ultima_vistoria** | INTEGER | NULLABLE | Ano atualizado da √∫ltima vistoria realizada pelo ve√≠culo. |\n",
        "| **ultima_situacao** | STRING | NULLABLE | √öltima situa√ß√£o do ve√≠culo na data atual. |\n",
        "| **tecnologia** | STRING | NULLABLE | Tecnologia utilizada no ve√≠culo [BASICO, MIDI, MINI, PADRON, ARTICULADO]. |\n",
        "| **quantidade_lotacao_pe** | INTEGER | NULLABLE | Capacidade de passageiros em p√©. |\n",
        "| **quantidade_lotacao_sentado** | INTEGER | NULLABLE | Capacidade de passageiros sentados. |\n",
        "| **tipo_combustivel** | STRING | NULLABLE | Tipo de combust√≠vel utilizado. |\n",
        "| **indicador_ar_condicionado** | BOOLEAN | NULLABLE | Indicador se possui ar condicionado [Verdadeiro/Falso]. |\n",
        "| **indicador_elevador** | BOOLEAN | NULLABLE | Indicador se possui elevador [Verdadeiro/Falso]. |\n",
        "| **indicador_usb** | BOOLEAN | NULLABLE | Indicador se tem USB [Verdadeiro/Falso]. |\n",
        "| **indicador_wifi** | BOOLEAN | NULLABLE | Indicador se tem Wi-fi [Verdadeiro/Falso]. |\n",
        "| **indicador_veiculo_lacrado** | BOOLEAN | NULLABLE | Indicador se o ve√≠culo estava lacrado na data atual [Verdadeiro/Falso]. |\n",
        "| **indicador_data_ultima_vistoria_tratada** | BOOLEAN | NULLABLE | Indica se a data da √∫ltima vistoria original passou por algum processo de tratamento, limpeza ou normaliza√ß√£o. |\n",
        "| **data_arquivo_fonte** | DATE | NULLABLE | Data do arquivo do STU com os dados utilizados. |\n",
        "| **versao** | STRING | NULLABLE | C√≥digo de controle de vers√£o [SHA do GitHub]. |\n",
        "| **datetime_ultima_atualizacao** | DATETIME | NULLABLE | √öltima atualiza√ß√£o [GMT-3]. |\n",
        "| **id_execucao_dbt** | STRING | NULLABLE | Identificador da execu√ß√£o do DBT que modificou o dado pela √∫ltima vez. |\n",
        "| **ano_ultima_vistoria_atualizado** | INTEGER | NULLABLE | Ano correspondente √† √∫ltima vistoria, recalculado ou corrigido ap√≥s processos de tratamento de dados. |\n"
      ],
      "metadata": {
        "id": "3KD-Yl95eZYI"
      },
      "id": "3KD-Yl95eZYI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**: *transporte_rodoviario_municipal*\n",
        "### **Tabela**: *viagem_onibus*\n",
        "### **Descri√ß√£o**: Essa √© a tabela na qual temos as informa√ß√µes das viagens de todos os √¥nibus que circulam na Cidade do Rio de Janeiro.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **data** | DATE | NULLABLE | Data da viagem. |\n",
        "| **consorcio** | STRING | NULLABLE | Cons√≥rcio ao qual o servi√ßo pertence. |\n",
        "| **tipo_dia** | STRING | NULLABLE | Dia da semana - categorias: Dia √ötil, S√°bado, Domingo e Ponto Facultativo. |\n",
        "| **id_empresa** | STRING | NULLABLE | C√≥digo identificador da empresa que opera o ve√≠culo. |\n",
        "| **id_veiculo** | STRING | NULLABLE | C√≥digo identificador do ve√≠culo [n√∫mero de ordem]. |\n",
        "| **id_viagem** | STRING | NULLABLE | C√≥digo √∫nico identificador da viagem. |\n",
        "| **servico** | STRING | NULLABLE | Servi√ßo realizado pelo ve√≠culo [com base na identifica√ß√£o do trajeto]. |\n",
        "| **shape_id** | STRING | NULLABLE | C√≥digo identificador do shape [trajeto]. |\n",
        "| **sentido** | STRING | NULLABLE | Sentido da linha. |\n",
        "| **datetime_partida** | DATETIME | NULLABLE | Hor√°rio de in√≠cio da viagem. |\n",
        "| **datetime_chegada** | DATETIME | NULLABLE | Hor√°rio de fim da viagem. |\n",
        "| **tempo_viagem** | INTEGER | NULLABLE | Tempo aferido da viagem (minutos). |\n",
        "| **distancia_planejada** | FLOAT | NULLABLE | Dist√¢ncia do shape [trajeto] planejado (km). |\n",
        "| **perc_conformidade_shape** | FLOAT | NULLABLE | Percentual de sinais emitidos dentro do shape [trajeto] ao longo da viagem. |\n",
        "| **perc_conformidade_registros** | FLOAT | NULLABLE | Percentual de minutos da viagem com registro de sinal de GPS. |\n",
        "| **versao_modelo** | STRING | NULLABLE | C√≥digo de controle de vers√£o [SHA do GitHub]. |"
      ],
      "metadata": {
        "id": "QDKORFPihRqi"
      },
      "id": "QDKORFPihRqi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Essas s√£o as tabelas que ser√£o convertidas em outros conjuntos de dados para gerar os insights e responder as quest√µes presentes no *objetivo* desse trabalho."
      ],
      "metadata": {
        "id": "3c36vmyfiKkC"
      },
      "id": "3c36vmyfiKkC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coleta e Ingest√£o de Dados para a Camada Bronze**"
      ],
      "metadata": {
        "id": "4cZaAfMvGaCv"
      },
      "id": "4cZaAfMvGaCv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Origem dos Dados e Desafios Iniciais**\n",
        "Devido a restri√ß√µes de acesso via Conta de Servi√ßo ao Google BigQuery do **data.rio**, a estrat√©gia de coleta foi adaptada. Realizamos a extra√ß√£o manual das tabelas para arquivos **.csv** (limitados a 1 GB cada) e o upload para uma estrutura organizada no **Google Drive**.\n",
        "\n",
        "![Estrutura de Pastas com arquivos coletados](https://drive.google.com/uc?id=13RVLAW-ASzXjo-M8gAUSJbcO7OZtEr7W)\n",
        "\n"
      ],
      "metadata": {
        "id": "FHjsq_FsGeYj"
      },
      "id": "FHjsq_FsGeYj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O volume total coletado √© de **6,71 GB**. Embora modesto para padr√µes de Big Data, este volume √© ideal para demonstrar a efici√™ncia de ferramentas *open source* em processar dados que superam a capacidade de mem√≥ria convencional de m√°quinas locais, utilizando t√©cnicas de *Lazy Loading* e *Streaming*.\n",
        "\n",
        "![Estrutura de dentro de uma pasta](https://drive.google.com/uc?id=1irSDz-fItos9fnblISWjphs-3LyxAOpd)"
      ],
      "metadata": {
        "id": "91jXPpE_GmJ_"
      },
      "id": "91jXPpE_GmJ_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Orquestra√ß√£o com Apache Airflow**\n",
        "### Para automatizar a movimenta√ß√£o desses dados para a nuvem, utilizamos o **Apache Airflow** rodando em uma VPS via Docker. O Airflow √© uma plataforma de c√≥digo aberto para agendar e monitorar fluxos de trabalho.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1cRHfKNydWC-uFUzitbJgXqJL-zTyML5O\" alt=\"Arquitetura do Airflow\" width=\"700\">\n",
        "\n",
        "\n",
        "* ### **O que √© uma DAG?** No Airflow, o pipeline √© definido como uma **DAG (Directed Acyclic Graph)**. Trata-se de um conjunto de tarefas organizadas de forma que reflitam suas depend√™ncias e rela√ß√µes, garantindo que o fluxo de dados siga uma dire√ß√£o l√≥gica sem loops infinitos.\n",
        "* ### **Pipeline de Ingest√£o (Factory Pattern):** Nossa DAG foi desenvolvida utilizando o padr√£o *Factory* (Orchestrator). Isso permite que novos datasets sejam adicionados apenas via configura√ß√£o, garantindo que a orquestra√ß√£o cres√ßa sem a necessidade de reescrever o c√≥digo principal."
      ],
      "metadata": {
        "id": "XKoPC7mVGnx8"
      },
      "id": "XKoPC7mVGnx8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Componentes do Diret√≥rio `dags/`**\n",
        "\n",
        "### A l√≥gica do pipeline est√° segmentada para garantir modularidade, seguindo princ√≠pios de arquitetura de software:\n",
        "\n",
        "* ### **Entrypoint**: √â o [arquivo](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/mvp_ingestion_pipeline.py) de entrada que instancia o `PipelineOrchestrator` e exp√µe o objeto `dag` para o Airflow.\n",
        "* ### [**Classe** `PipelineOrchestrator`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/modules/mvp_orchestrator_dag.py) : Respons√°vel por iterar sobre os datasets e criar dinamicamente as tarefas e depend√™ncias. Aqui implementamos a ordem cr√≠tica de execu√ß√£o: **Branch -> Ingest√£o -> Registro -> Merge**.\n",
        "* ### [**Classe** `IngestionManager`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/modules/ingestion_manager.py): O motor de processamento. Utiliza o **DuckDB** para converter CSV em Parquet e realiza valida√ß√µes de vacuidade antes do upload.\n",
        "* ### [**Classe** `DataOpsManager`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/modules/dataops_manager.py): O componente de governan√ßa. Gerencia o **Project Nessie** via ODBC, criando branches isoladas para o ETL e garantindo o registro de tabelas Iceberg no Dremio.\n",
        "\n",
        "### Essa separa√ß√£o garante que altera√ß√µes na l√≥gica de transforma√ß√£o (DuckDB) ou na governan√ßa (Nessie) n√£o quebrem a estrutura da orquestra√ß√£o."
      ],
      "metadata": {
        "id": "P6Kt3KqzG0Dv"
      },
      "id": "P6Kt3KqzG0Dv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Fluxo de Dados e Padr√£o WAP (Write-Audit-Publish)**\n",
        "### O processo segue o padr√£o rigoroso de **WAP** para garantir que a ramifica√ß√£o principal (*main*) nunca receba dados corrompidos:\n",
        "\n",
        "1.  ### **Isolamento (Write):** O `DataOpsManager` cria uma branch tempor√°ria no Nessie (ex: `dev_20251220`). Toda a escrita ocorre neste ambiente isolado.\n",
        "2.  ### **Ingest√£o e Convers√£o:** O DuckDB l√™ os CSVs em *streaming* e gera arquivos **Parquet** com tipagem autom√°tica.\n",
        "3.  ### **Auditoria Bronze (Audit):** Antes do upload, validamos se o arquivo n√£o est√° vazio. Na camada Bronze, o dado √© mantido em formato *Raw* para preservar a linhagem completa.\n",
        "4.  ### **Registro e Publica√ß√£o (Publish):** Registramos a tabela no Dremio apontando para o GCS simplificado (`dataset/ano=YYYY/mes=MM/dia=DD`). Ap√≥s o sucesso, realizamos o **Merge** at√¥mico para a branch `main`."
      ],
      "metadata": {
        "id": "TqLvEA4DG4w6"
      },
      "id": "TqLvEA4DG4w6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Seguran√ßa e Governan√ßa (IAM)**\n",
        "### A seguran√ßa √© baseada no princ√≠pio do **Privil√©gio M√≠nimo** via **IAM**:\n",
        "\n",
        "* ### **Service Account (SA):** Identidade exclusiva para o Airflow com chaves JSON criptografadas.\n",
        "* ### **Escopo:** Leitura restrita no Drive e permiss√£o `Storage Object Admin` nos buckets de Landing e Bronze.\n",
        "\n",
        "> ### **Nota de Arquiteto:** Nenhuma credencial √© exposta. Configura√ß√µes de IDs e Buckets s√£o injetadas via vari√°veis de ambiente protegidas no container."
      ],
      "metadata": {
        "id": "aJsfpWoKG-oA"
      },
      "id": "aJsfpWoKG-oA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Reposit√≥rio de C√≥digo**\n",
        "### Toda a l√≥gica de orquestra√ß√£o e as classes de gerenciamento est√£o versionadas no GitHub:\n",
        "\n",
        "> üîó [**Acessar Diret√≥rio das DAGs no GitHub**](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/tree/master/dags)"
      ],
      "metadata": {
        "id": "hFOwnACZHCS3"
      },
      "id": "hFOwnACZHCS3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Detalhes T√©cnicos: Implementando o Bypass**\n",
        "### Enfrentamos um grande desafio na comunica√ß√£o do Airflow com o Google Drive. Como mostra o v√≠deo abaixo:"
      ],
      "metadata": {
        "id": "xC75HXylHF1h"
      },
      "id": "xC75HXylHF1h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1Mow0LfPljsZs_S4YuHADUR18bS7RGZxE\" alt=\"Talk is cheap, show me the code\" width=\"500\">\n",
        "<center />"
      ],
      "metadata": {
        "id": "TpSt9wLwHJ2t"
      },
      "id": "TpSt9wLwHJ2t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Para resolver, implementamos um *bypass* direto via Google Discovery API no `IngestionManager`, substituindo o Hook padr√£o por uma solu√ß√£o mais est√°vel:\n",
        "\n",
        "### **Antes (Padr√£o Airflow):**\n",
        "```python\n",
        "from airflow.providers.google.cloud.hooks.drive import GoogleDriveHook\n",
        "\n",
        "def __init__(self):\n",
        "    self._drive_hook = GoogleDriveHook(gcp_conn_id=\"google_cloud_default\")\n",
        "\n",
        "def list_files(self, folder_id):\n",
        "    return self._drive_hook.get_conn().files().list(q=f\"'{folder_id}' in parents\").execute()\n",
        "```\n",
        "\n",
        "### **Depois (Bypass Direto - IngestionManager):**\n",
        "```python\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "def __init__(self):\n",
        "    self._key_path = \"/opt/airflow/keys/sa-airflow.json\"\n",
        "    self._scopes = ['[https://www.googleapis.com/auth/drive.readonly](https://www.googleapis.com/auth/drive.readonly)']\n",
        "\n",
        "def _get_drive_service(self):\n",
        "    creds = service_account.Credentials.from_service_account_file(\n",
        "        self._key_path, scopes=self._scopes\n",
        "    )\n",
        "    return build('drive', 'v3', credentials=creds)\n",
        "```\n",
        "### **Integra√ß√£o com Dremio (DataOpsManager)**\n",
        "### Para finalizar a camada Bronze, automatizamos o registro das tabelas como Apache Iceberg, permitindo que o Dremio gerencie vers√µes de forma at√¥mica:\n",
        "```python\n",
        "def create_dag(self, dag_id: str, schedule_interval):\n",
        "        with DAG(\n",
        "            dag_id=dag_id,\n",
        "            default_args=self._default_args,\n",
        "            description=\"Pipeline de Ingest√£o: Drive -> Landing -> Bronze (Parquet)\",\n",
        "            schedule_interval=schedule_interval,\n",
        "            catchup=False,\n",
        "            tags=[\"ingestao\", \"bronze\", \"duckdb\", \"poo\"],\n",
        "        ) as dag:\n",
        "\n",
        "            # 1. Loop de Datasets (Configurado sem as reclama√ß√µes)\n",
        "            for dataset_name, folder_id in self._datasets_config.items():\n",
        "\n",
        "                if not folder_id:\n",
        "                    continue\n",
        "\n",
        "                # 2. Branch √öNICA por Dataset (Ex: dev_viagens_onibus_20251220)\n",
        "                # Isso isola o trabalho e evita que um erro \"apague\" o progresso de outros\n",
        "                branch_name = f\"dev_{dataset_name}_{{{{ ds_nodash }}}}\"\n",
        "\n",
        "                # 2.1 Criar a branch para este dataset\n",
        "                create_branch_task = PythonOperator(\n",
        "                    task_id=f\"create_branch_{dataset_name}\",\n",
        "                    python_callable=self._dataops.create_branch,\n",
        "                    op_kwargs={\"branch_name\": branch_name},\n",
        "                )\n",
        "\n",
        "                # 2.2 Ingest√£o (Drive -> GCS)\n",
        "                ingest_task = PythonOperator(\n",
        "                    task_id=f\"ingest_{dataset_name}\",\n",
        "                    python_callable=self._ingestor.process_file_to_bronze,\n",
        "                    op_kwargs={\n",
        "                        \"folder_id\": folder_id,\n",
        "                        \"bucket_landing\": self._bucket_landing,\n",
        "                        \"bucket_bronze\": self._bucket_bronze,\n",
        "                        \"dataset_name\": dataset_name,\n",
        "                        \"data_referencia\": \"{{ ds }}\",\n",
        "                    },\n",
        "                )\n",
        "\n",
        "                # 2.3 Registro (Dremio/Nessie) com Tipagem Forte\n",
        "                register_task = PythonOperator(\n",
        "                    task_id=f\"register_{dataset_name}_on_dremio\",\n",
        "                    python_callable=self._dataops.ensure_table_exists,\n",
        "                    op_kwargs={\n",
        "                        \"dataset_name\": dataset_name,\n",
        "                        \"branch_name\": branch_name,\n",
        "                        \"bucket_name\": self._bucket_bronze,\n",
        "                        \"schema_string\": SCHEMAS.get(dataset_name),\n",
        "                    },\n",
        "                )\n",
        "\n",
        "                # 2.4 Merge Individual (Publica apenas este dataset na Main)\n",
        "                merge_task = PythonOperator(\n",
        "                    task_id=f\"merge_{dataset_name}_to_main\",\n",
        "                    python_callable=self._dataops.merge_branch,\n",
        "                    op_kwargs={\"branch_name\": branch_name, \"target_ref\": \"main\"},\n",
        "                )\n",
        "\n",
        "                # Define o fluxo isolado: Branch -> Ingest -> Register -> Merge\n",
        "                create_branch_task >> ingest_task >> register_task >> merge_task\n",
        "\n",
        "            return dag\n",
        "```\n",
        "```python\n",
        "class DataOpsManager:\n",
        "    def __init__(self, conn_id=\"dremio_odbc\", catalog=\"nessie_catalog\") -> None:\n",
        "        self._conn_id = conn_id\n",
        "        self._catalog = catalog\n",
        "        self._hook = OdbcHook(odbc_conn_id=conn_id)\n",
        "\n",
        "    def _execute_sql_direct(self, sql: str):\n",
        "        \"\"\"\n",
        "        Bypass Spartan: For√ßa o autocommit=True em todas as execu√ß√µes.\n",
        "        Impede que o Airflow tente desabilitar o autocommit (causador do HYC00).\n",
        "        \"\"\"\n",
        "        conn_str = self._hook.odbc_connection_string\n",
        "        logging.info(f\"Dremio SQL Exec: {sql}\")\n",
        "\n",
        "        with pyodbc.connect(conn_str, autocommit=True) as conn:\n",
        "            with conn.cursor() as cursor:\n",
        "                cursor.execute(sql)\n",
        "\n",
        "    def create_branch(self, branch_name: str, source_ref=\"main\"):\n",
        "        sql = f'CREATE BRANCH \"{branch_name}\" IN {self._catalog}'\n",
        "        try:\n",
        "            self._execute_sql_direct(sql)\n",
        "            logging.info(f\"Branch '{branch_name}' pronta.\")\n",
        "        except Exception as e:\n",
        "            if \"already exists\" in str(e).lower():\n",
        "                logging.warning(f\"Branch '{branch_name}' j√° existe.\")\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    def ensure_table_exists(\n",
        "        self,\n",
        "        dataset_name: str,\n",
        "        branch_name: str,\n",
        "        bucket_name: str,\n",
        "        schema_string: str = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Cria a tabela no Nessie usando CAST para garantir os tipos do cat√°logo.\n",
        "        \"\"\"\n",
        "        use_ref = f'USE REFERENCE \"{branch_name}\" IN {self._catalog}'\n",
        "\n",
        "        if schema_string:\n",
        "            # Transforma \"col TIPO\" em \"CAST(col AS TIPO) as col\"\n",
        "            cols_with_types = [c.strip() for c in schema_string.split(\",\")]\n",
        "            cast_list = []\n",
        "            for item in cols_with_types:\n",
        "                parts = item.split()\n",
        "                if len(parts) >= 2:\n",
        "                    col_name = parts[0]\n",
        "                    col_type = parts[1]\n",
        "                    cast_list.append(f\"CAST({col_name} AS {col_type}) as {col_name}\")\n",
        "\n",
        "            select_clause = \", \".join(cast_list)\n",
        "        else:\n",
        "            select_clause = \"*\"\n",
        "\n",
        "        # Criamos via CTAS. O Dremio gerencia o LOCATION automaticamente no Nessie.\n",
        "        sql_create = f\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS {self._catalog}.{dataset_name}\n",
        "            AS SELECT {select_clause}\n",
        "            FROM TABLE(gcs_bronze.\"{bucket_name}\".\"{dataset_name}\" (type => 'parquet'))\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            logging.info(f\"Registrando {dataset_name} com tipagem expl√≠cita...\")\n",
        "            self._execute_sql_direct(use_ref)\n",
        "            self._execute_sql_direct(sql_create)\n",
        "            logging.info(f\"Tabela {dataset_name} criada com sucesso.\")\n",
        "        except Exception as e:\n",
        "            if \"already exists\" in str(e).lower():\n",
        "                logging.warning(f\"Tabela {dataset_name} j√° existe na branch.\")\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    def merge_branch(self, branch_name: str, target_ref=\"main\"):\n",
        "        \"\"\"\n",
        "        Publica os dados da branch de dev para a main.\n",
        "        \"\"\"\n",
        "        sql = f'MERGE BRANCH \"{branch_name}\" INTO \"{target_ref}\" IN {self._catalog}'\n",
        "        try:\n",
        "            logging.info(f\"Publicando dados: {branch_name} -> {target_ref}\")\n",
        "            self._execute_sql_direct(sql)\n",
        "            logging.info(\"Merge conclu√≠do!\")\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "```"
      ],
      "metadata": {
        "id": "SG-SazynHMmu"
      },
      "id": "SG-SazynHMmu"
    },
    {
      "cell_type": "markdown",
      "id": "4e4ab599",
      "metadata": {
        "id": "4e4ab599"
      },
      "source": [
        "### **Evolu√ß√£o da Estrat√©gia de Ingest√£o: Do Funil ao Isolamento**\n",
        "### Inicialmente, o pipeline foi desenhado em um modelo de \"funil\", onde todos os datasets (viagens, clima, frota e reclama√ß√µes) convergiam para uma √∫nica branch de desenvolvimento antes do merge final para a produ√ß√£o (`main`).\n",
        "\n",
        "* ### **O Problema (Gargalo)**: Como demonstrado na Figura 1, uma falha em um √∫nico dataset (neste caso, `reclamacoes_1746`) causou o erro `upstream_failed` na tarefa de publica√ß√£o final. Isso impedia que mesmo os dados saud√°veis chegassem aos usu√°rios finais, caracterizando um ponto √∫nico de falha.\n",
        "\n",
        "* ### **A Solu√ß√£o (Isolamento de Branches)**: A arquitetura foi refatorada para um modelo de \"trilhos paralelos\" (Video abaixo). Cada dataset passou a ter seu pr√≥prio ciclo de vida completo: cria√ß√£o de branch dedicada, ingest√£o, registro e merge individual.\n",
        "### **Foram 10 horas de um dia para resolver todos os problemas de integra√ß√£o entre o Airflow, Dremio e Nessie.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1eHWavX9H0tbxpGu1qYbnrhPxT2phiwOt\" alt=\"Pipeline do Airflow executado com sucesso!\">\n",
        "<center />\n",
        "\n",
        "*Arquitetura final do pipeline de dados no Apache Airflow, demonstrando o paralelismo total e o isolamento transacional via branches do Project Nessie para cada entidade do dom√≠nio de transportes e clima.*"
      ],
      "metadata": {
        "id": "xSNsnDGzXbb-"
      },
      "id": "xSNsnDGzXbb-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ciclo de Vida do Dado no Lakehouse**\n",
        "### A estrutura vis√≠vel da grava√ß√£o acima detalha as quatro etapas cr√≠ticas para garantir a integridade e a disponibilidade de cada tabela. Vamos passear por cada uma delas com o devido c√≥digo fonte:\n"
      ],
      "metadata": {
        "id": "vP82b3zTpIJ-"
      },
      "id": "vP82b3zTpIJ-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Cria√ß√£o de Branch Ef√™mera (`create_branch`)**: Para cada execu√ß√£o, o Project Nessie cria uma ramifica√ß√£o isolada (ex: `dev_clima_pluviometria_20251220`). Isso permite que opera√ß√µes de escrita ocorram sem risco de corrup√ß√£o da branch principal.\n",
        "---\n",
        "\n",
        "```python\n",
        "def create_branch(self, branch_name: str, source_ref=\"main\"):\n",
        "        sql = f'CREATE BRANCH \"{branch_name}\" IN {self._catalog}'\n",
        "        try:\n",
        "            self._execute_sql_direct(sql)\n",
        "            logging.info(f\"Branch '{branch_name}' pronta.\")\n",
        "        except Exception as e:\n",
        "            if \"already exists\" in str(e).lower():\n",
        "                logging.warning(f\"Branch '{branch_name}' j√° existe.\")\n",
        "            else:\n",
        "                raise e\n",
        "```\n",
        "*Trecho de c√≥digo pertencente a [classe `DataOpsManager`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/modules/dataops_manager.py)*\n",
        "\n",
        "\n",
        "```python\n",
        "create_branch_task = PythonOperator(\n",
        "                    task_id=f\"create_branch_{dataset_name}\",\n",
        "                    python_callable=self._dataops.create_branch,\n",
        "                    op_kwargs={\"branch_name\": branch_name},\n",
        "                )\n",
        "```\n",
        "*Trecho de c√≥digo pertence a [classe `PipelineOrchestrator`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/modules/mvp_orchestrator_dag.py)*"
      ],
      "metadata": {
        "id": "WGOqB02gnDCH"
      },
      "id": "WGOqB02gnDCH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![GitDataBranch](https://drive.google.com/uc?id=1uymNNpOhkOJHlcP1WZ_iJxok8a2fPRNm)\n",
        "\n",
        "*Interface de gerenciamento do Project Nessie exibindo o hist√≥rico cronol√≥gico de commits. Cada entrada representa uma opera√ß√£o at√¥mica de DDL (Data Definition Language) ou DML (Data Manipulation Language), assegurando a auditoria completa e a capacidade de reprodutibilidade temporal (Time Travel) sobre o cat√°logo Iceberg.*"
      ],
      "metadata": {
        "id": "-CerosuMp7J0"
      },
      "id": "-CerosuMp7J0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Diagrama de Linhagem: Modelo Git Flow para Dados**"
      ],
      "metadata": {
        "id": "SjpOg8gZsnmj"
      },
      "id": "SjpOg8gZsnmj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Podemos representar o fluxo de trabalho de dados para a camada Bronze com o seguinte grafo, que ilustra o paralelismo das branches individuais:\n",
        "\n",
        "* ### **Branches Ef√™meras por Dataset**: Diferente de um fluxo Git tradicional onde as branches duram dias, no seu Lakehouse as branches s√£o ef√™meras e criadas programaticamente pelo Airflow para cada tarefa de ingest√£o. Isso garante que a branch `main` nunca seja exposta a estados intermedi√°rios ou falhas de schema.\n",
        "\n",
        "* ### **Commits de DDL (Data Definition Language)**: Cada \"bolinha\" no grafo (commit) representa a materializa√ß√£o f√≠sica dos metadados Iceberg. No seu caso, esses commits carregam a **tipagem forte** (CAST) que voc√™ definiu no cat√°logo, garantindo que o dado na `main` seja imut√°vel e confi√°vel.\n",
        "\n",
        "* ### **Merge At√¥mico**: As linhas que voltam para a `main` representam o momento em que o dado se torna vis√≠vel para o Dremio. Como demonstrado na Figura anterior, o Nessie registra o hash de cada merge, permitindo que voc√™ fa√ßa o \"Time Travel\" (viagem no tempo) para qualquer ponto desse gr√°fico.\n",
        "\n",
        "![Git-Graph dos dados](https://drive.google.com/uc?id=1U72uFHAGYk_hnNprAXydVCzClD_Qckr3)\n",
        "\n",
        "*Grafo de versionamento do cat√°logo Nessie. A estrutura demonstra o isolamento transacional onde cada dataset (Clima, Viagens, Frota, Esta√ß√µes) opera em uma ramifica√ß√£o independente, culminando em merges at√¥micos na branch principal (main) ap√≥s a valida√ß√£o do esquema.*"
      ],
      "metadata": {
        "id": "O99C_xOss-kD"
      },
      "id": "O99C_xOss-kD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Ingest√£o via Edge ETL (`ingest_task`)**: Processamento local utilizando DuckDB para transformar arquivos brutos em Parquet otimizado no Google Cloud Storage (GCS).\n",
        "\n",
        "```python\n",
        "ingest_task = PythonOperator(\n",
        "                    task_id=f\"ingest_{dataset_name}\",\n",
        "                    python_callable=self._ingestor.process_file_to_bronze,\n",
        "                    op_kwargs={\n",
        "                        \"folder_id\": folder_id,\n",
        "                        \"bucket_landing\": self._bucket_landing,\n",
        "                        \"bucket_bronze\": self._bucket_bronze,\n",
        "                        \"dataset_name\": dataset_name,\n",
        "                        \"data_referencia\": \"{{ ds }}\",\n",
        "                    },\n",
        "                )\n",
        "```\n",
        "*Trecho de c√≥digo pertence a [classe `PipelineOrchestrator`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/modules/mvp_orchestrator_dag.py)*"
      ],
      "metadata": {
        "id": "IZlPh1iUpPON"
      },
      "id": "IZlPh1iUpPON"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Registro com Schema Enforcement (`register_task`)**: A tabela √© materializada no cat√°logo atrav√©s de comandos `CREATE TABLE AS SELECT` com tipagem forte (`CAST`). Isso resolve o problema de inconsist√™ncia de tipos observado anteriormente.\n",
        "\n",
        "```python\n",
        "register_task = PythonOperator(\n",
        "                    task_id=f\"register_{dataset_name}_on_dremio\",\n",
        "                    python_callable=self._dataops.ensure_table_exists,\n",
        "                    op_kwargs={\n",
        "                        \"dataset_name\": dataset_name,\n",
        "                        \"branch_name\": branch_name,\n",
        "                        \"bucket_name\": self._bucket_bronze,\n",
        "                        \"schema_string\": SCHEMAS.get(dataset_name),\n",
        "                    },\n",
        "                )\n",
        "```\n",
        "*Trecho de c√≥digo pertence a [classe `PipelineOrchestrator`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/modules/mvp_orchestrator_dag.py)*"
      ],
      "metadata": {
        "id": "6qqqEexXyDFi"
      },
      "id": "6qqqEexXyDFi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Merge At√¥mico (`merge_task`)**: Assim que o registro √© validado, a branch √© mesclada na `main`, tornando o dado dispon√≠vel para consumo no Dremio e PowerBI de forma instant√¢nea e segura.\n",
        "\n",
        "```python\n",
        "merge_task = PythonOperator(\n",
        "                    task_id=f\"merge_{dataset_name}_to_main\",\n",
        "                    python_callable=self._dataops.merge_branch,\n",
        "                    op_kwargs={\"branch_name\": branch_name, \"target_ref\": \"main\"},\n",
        "                )\n",
        "```\n",
        "*Trecho de c√≥digo pertence a [classe `PipelineOrchestrator`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/modules/mvp_orchestrator_dag.py)*"
      ],
      "metadata": {
        "id": "-6pnkXi4z9Ft"
      },
      "id": "-6pnkXi4z9Ft"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Benef√≠cios T√©cnicos Implementados**\n",
        "\n",
        "* ### **Versionamento Nativo (Git-for-Data)**: Atrav√©s do Nessie, o hist√≥rico de cada altera√ß√£o √© registrado como um commit, permitindo auditoria total e a possibilidade de realizar *rollbacks* se necess√°rio.\n",
        "\n",
        "* ### **Disponibilidade Parcial**: Mesmo que um dataset apresente erros, as outras esteiras continuam operacionais, garantindo que o Lakehouse seja atualizado com os dados dispon√≠veis.\n",
        "\n",
        "* ### **Consist√™ncia ACID**: O uso do formato Apache Iceberg garante que as opera√ß√µes de escrita e leitura sejam consistentes, evitando que usu√°rios vejam dados parciais durante o processo de ingest√£o."
      ],
      "metadata": {
        "id": "ZDvRHplS0r6G"
      },
      "id": "ZDvRHplS0r6G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E isso foi decisivo para que os dados fossem entregues corretamente no Dremio, como mostra a figura abaixo:"
      ],
      "metadata": {
        "id": "7C95_NUKH4YO"
      },
      "id": "7C95_NUKH4YO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?id=1HbXyNpOWtWo8fqiGuGanUU7onz_EHW29\" alt=\"Tabelas criadas no Dremio\">\n",
        "<center />"
      ],
      "metadata": {
        "id": "xVA4-iaRmRfm"
      },
      "id": "xVA4-iaRmRfm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problemas estruturais ap√≥s a cria√ß√£o da Camada Bronze**"
      ],
      "metadata": {
        "id": "ia6OA5tnmonl"
      },
      "id": "ia6OA5tnmonl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tivemos grandes problemas com o ambiente e com isso, tivemos de eliminar o dbt e o Nessie do MVP, motivos dos quais s√≥ vamos verificar ap√≥s a entrega, vide a se√ß√£o de Autoavalia√ß√£o para informa√ß√µes adicionais sobre o \"Apocalipse\" que ocorreu na v√©spera da entrega.\n",
        "### Embora o pipeline de ingest√£o tenha sido validado com sucesso via Airflow at√© a persist√™ncia no Google Cloud Storage, a orquestra√ß√£o do cat√°logo de metadados foi realizada manualmente no motor Dremio para este MVP. Esta decis√£o t√©cnica visou garantir a integridade dos tipos de dados (Data Quality) antes da transi√ß√£o para a camada Prata. Em arquiteturas futuras, prop√µe-se a integra√ß√£o de cat√°logos automatizados (como Project Nessie) operando em bancos de dados transacionais persistentes (PostgreSQL) para suportar o versionamento de dados em escala.\n",
        "### Criamos queries que seguem o padr√£o **CTAS** (*Create Table as Select*) onde vamos execut√°-las diretamente sobre o Dremio com o Apache Iceberg.\n",
        "### Tivemos de recriar toda a estrutura da camada Bronze novamente, mas dentro do dremio, simulando o que fazia o Nessie por quest√µes de compatibilidade e fidelidade ao MVP.\n",
        "### Para tal, criamos uma fonte de dados no Dremio apontando para o bucket da camada Bronze, como mostra a figura abaixo:\n",
        "\n",
        "![gcs_bronze](https://drive.google.com/uc?id=1_rA6wiKXvH5fbod4Z8r_Ih3PhQXBgxYJ)\n",
        "\n",
        "### Com a op√ß√£o CTAS selecionada para *ICEBERG*\n",
        "![CTAS Habilitado](https://drive.google.com/uc?id=1EsJpcQG75u3qwgqJbRagNBL6eO5WeZC2)\n",
        "\n",
        "### Tivemos de criar uma nova estrutura em nosso projeto para receber esses scripts .sql:\n",
        "\n",
        "```plaintext\n",
        "~/mvp_transporte/\n",
        "‚îú‚îÄ‚îÄ sql/                   # Scripts SQL organizados por camada\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ bronze/            # Queries CTAS (as que passei acima)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ silver/            # Queries de limpeza e enriquecimento\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ gold/              # Queries de agrega√ß√£o para dashboards\n",
        "‚îî‚îÄ‚îÄ docs/                  # Documenta√ß√£o do projeto\n",
        "    ‚îî‚îÄ‚îÄ catalog/           # Dicion√°rio de dados e defini√ß√µes de tabelas\n",
        "```\n",
        "### **Queries CTAS: Criando a Camada Bronze (Iceberg)**\n",
        "\n",
        "Diferente da leitura direta do Parquet, o CTAS cria uma tabela gerenciada com metadados Iceberg, o que permite versionamento e performance superiores para a camada Prata. Utilizaremos os esquemas definidos anteriormente para garantir a tipagem forte.\n",
        "\n",
        "### Criamos uma para cada tabela baseado no cat√°logo de dados do provedor dos dados, que, inclusive, j√° mostramos aqui em se√ß√µes anteriores.\n",
        "\n",
        "### Escolhemos o namespace `mvp_transporte.bronze` ao inv√©s de `nessie_catalog` como dantes.\n",
        "\n",
        "### Para cada query ser√° criado um arquivo com extens√£o .sql para documenta√ß√£o.\n",
        "\n",
        "### [**Tabela**: `mvp_transporte.bronze.viagens_onibus_iceberg`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/sql/bronze/viagens_onibus.sql)\n",
        "### [**Tabela** : `mvp_transporte.bronze.clima_pluviometria_iceberg`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/sql/bronze/clima_pluviometria.sql)\n",
        "### [**Tabela**: `mvp_transporte.bronze.estacoes_clima_iceberg`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/sql/bronze/estacoes_clima.sql)\n",
        "\n",
        "### **Arquitetura de Cat√°logo e Persist√™ncia no Dremio**\n",
        "### 1. **O Space: A Camada de Organiza√ß√£o Virtual**\n",
        "\n",
        "### Um **Space** no Dremio (ex: `mvp_transportes`) √© uma abstra√ß√£o l√≥gica, funcionando como um Namespace. Entretanto, um Space n√£o possui armazenamento f√≠sico pr√≥prio; ele n√£o tem um disco r√≠gido ou um bucket atrelado a ele.\n",
        "\n",
        "* ### Por que criamos? Para organizar o projeto de forma hier√°rquica (Bronze, Silver, Gold), permitindo aplicar governan√ßa e facilitar a descoberta de dados sem expor a complexidade dos caminhos f√≠sicos do Google Cloud Storage.\n",
        "\n",
        "### **A Tabela F√≠sica no Bucket: Persist√™ncia e Performance**\n",
        "### Diferente dos Spaces, os **Sources** ou **Fontes de dados** (como a *gcs_bronze*) s√£o as conex√µes com o mundo real.\n",
        "\n",
        "* ### **Por que criar a tabela no Bucket?** Como o Dremio n√£o possui armazenamento, toda opera√ß√£o de escrita (`CREATE TABLE`) precisa de um destino f√≠sico que suporte arquivos.\n",
        "\n",
        "* ### Ao apontar para um storage qualquer, no nosso caso o bucket no GCS, o Dremio utiliza o formato **Iceberg** para gravar n√£o apenas os dados, mas tamb√©m os metadados (manifestos e vers√µes) que garantem a integridade da Camada Bronze.\n",
        "\n",
        "* ### Isso resolve o erro muito comum na cria√ß√£o de tabelas no Dremio de \"*container not found*\", pois agora o Dremio sabe exatamente em qual bucket/armazenamento f√≠sico deve depositar os arquivos.\n",
        "\n",
        "### Agora que temos as consultas, vamos execut√°-las e recome√ßar.\n",
        "\n",
        "### **A View: O Contrato de Dados (Decoupling)**\n",
        "### A cria√ß√£o de **Views** dentro dos Spaces √© a \"chave de ouro\" da arquitetura.\n",
        "* ### **Por que usar Views?** Uma View funciona como um **Contrato**. Ela aponta para a tabela f√≠sica complexa no GCS, mas oferece ao usu√°rio um nome simples e limpo dentro do Space (ex: `mvp_rio_transportes.bronze.viagens_onibus`).\n",
        "\n",
        "* ### **Vantagem T√©cnica**: Se no futuro voc√™ precisar trocar o bucket do Google Cloud ou migrar para a AWS, voc√™ altera apenas a defini√ß√£o da View. Para o analista ou para o dashboard, o nome da tabela permanece inalterado, garantindo **independ√™ncia de infraestrutura**.\n",
        "\n",
        "### A arquitetura implementada adotou o conceito de Camada Sem√¢ntica Desacoplada. Enquanto a persist√™ncia f√≠sica foi consolidada em tabelas gerenciadas no formato **Apache Iceberg** diretamente no Google Cloud Storage (garantindo durabilidade e performance), a interface de consumo foi organizada atrav√©s de **Virtual Spaces** no Dremio. Esta abordagem permite que a estrutura do projeto (Bronze, Silver e Gold) funcione como um cat√°logo l√≥gico independente do armazenamento f√≠sico, facilitando a manuten√ß√£o e a evolu√ß√£o do esquema de dados sem impactar as camadas superiores de consumo.\n",
        "\n",
        "![Tabelas na Camada Bronze](https://drive.google.com/uc?id=1aOWkwqhUBlXOtP_IOZ4Vb340QMj7agKK)\n",
        "\n",
        "*Tabelas na Camada Bronze dentro do Dremio*\n",
        "\n",
        "\n",
        "![Views dentro de um Space com um diret√≥rio chamado Bronze](https://drive.google.com/uc?id=1guyfm0gLUY5aFd1aYivayZaqR48mfvOm)\n",
        "\n",
        "*Views dentro de um Space com um diret√≥rio chamado Bronze*\n",
        "\n",
        "### Como podemos notar, existe uma diferen√ßa gr√°fica entre Tabelas e Views e isso facilita bastante o manuseio do ambiente e dos dados."
      ],
      "metadata": {
        "id": "A2PTeR1Csf1y"
      },
      "id": "A2PTeR1Csf1y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Camada Prata: tratamento dos dados, enriquecimento, cria√ß√£o de Esquema inicial e Carga**\n",
        "\n",
        "### A constru√ß√£o da camada Prata teve como objetivo central a cria√ß√£o de uma \"Single Source of Truth\" (SSOT) integrada, preparada para responder √†s pergunta presentes na se√ß√£o **Objetivo** deste MVP.\n",
        "\n",
        "### **O Caminho at√© a [`stg_viagens`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/sql/silver/stg_viagens.sql): Do Dado Bruto ao Dataset Curado**\n",
        "### A cria√ß√£o desta tabela foi o resultado de um processo de refinamento iterativo. Partimos de dados brutos na camada Bronze (telemetria e meteorologia) que estavam completamente desconectados em termos de gr√£o e tempo.\n",
        "\n",
        "### **1. Por que criamos esta tabela?**\n",
        "\n",
        "* ### **Integra√ß√£o de Dom√≠nios Distintos:** √înibus geram eventos a cada segundo; o clima gera medi√ß√µes a cada 15 minutos. Criamos a [`stg_viagens`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/sql/silver/stg_viagens.sql) para ser o \"ponto de encontro\", onde cada viagem √© carimbada com o estado do clima no momento exato da partida.\n",
        "\n",
        "* ### **Materializa√ß√£o de L√≥gica (Feature Engineering)**: Como voc√™ preza por **Clean Code**, movemos as regras de neg√≥cio complexas (como a classifica√ß√£o de intensidade de chuva e a detec√ß√£o de fins de semana) da visualiza√ß√£o para o processamento. Isso garante que a regra seja escrita uma √∫nica vez (DRY - *Don't Repeat Yourself*).\n",
        "\n",
        "* ### **Padroniza√ß√£o de Esquemas**: Passamos de arquivos Parquet sem tipagem r√≠gida para uma tabela **Iceberg**, garantindo integridade de tipos e facilitando a evolu√ß√£o do esquema no futuro.\n",
        "\n",
        "### **Por que a Wide Table √© a melhor escolha para a Camada Prata?**\n",
        "\n",
        "### Em arquiteturas de Data Lakehouse, a camada Prata atua como a **Staging Area** para a an√°lise. A conclus√£o de que uma **Wide Table (Tabela Larga)** √© o melhor modelo baseia-se em tr√™s pilares:\n",
        "\n",
        "### **A. Efici√™ncia Computacional (O(n) vs. O(n¬≤))**\n",
        "### A tentativa inicial de manter tabelas separadas e junt√°-las na hora da consulta gerou um **produto cartesiano** que travou o sistema por mais de 40 minutos.\n",
        "\n",
        "![Query levou muito tempo](https://drive.google.com/uc?id=1TcsfQ6XNY5tBMeBEHaGa7-DtUcK4FhDN)\n",
        "\n",
        "* ### **O Motivo**: Joins em tempo de execu√ß√£o sobre chaves n√£o √∫nicas e calculadas (como horas extra√≠das de datas) s√£o extremamente custosos.\n",
        "\n",
        "```sql\n",
        "CREATE TABLE \"gcs_silver\".stg_viagens AS\n",
        "SELECT\n",
        "    v.\"data\" as data_viagem,\n",
        "    v.id_viagem,\n",
        "    v.servico,\n",
        "    v.consorcio,\n",
        "    v.tipo_dia,\n",
        "    -- Campos para responder Perguntas 1 e 2 (Picos e Sazonalidade)\n",
        "    EXTRACT(HOUR FROM v.datetime_partida) as hora_partida,\n",
        "    CASE\n",
        "        WHEN EXTRACT(DOW FROM v.\"data\") IN (0, 6) THEN TRUE\n",
        "        ELSE FALSE\n",
        "    END as is_weekend,\n",
        "    -- Campos para responder Perguntas 3, 4 e 5 (Dura√ß√£o e Consist√™ncia)\n",
        "    v.tempo_viagem,\n",
        "    v.distancia_planejada,\n",
        "    -- Campos para responder Perguntas 5 e 6 (Impacto da Chuva)\n",
        "    COALESCE(c.chuva_1h_no_inicio, 0) as chuva_no_horario,\n",
        "    COALESCE(c.tipo_chuva, 'Sem Chuva') as condicao_clima\n",
        "FROM mvp_rio_transportes.bronze.viagens_onibus v\n",
        "LEFT JOIN mvp_rio_transportes.silver.viagens_com_clima c\n",
        "    ON v.\"data\" = c.data_particao\n",
        "    AND EXTRACT(HOUR FROM v.datetime_partida) = EXTRACT(HOUR FROM c.datetime_partida)\n",
        "```\n",
        "\n",
        "* ### **A Vantagem da Wide Table**: Ao denormalizar os dados agora, realizamos o Join pesado apenas uma vez. Para a camada Ouro, o custo de consulta ser√° linear, pois todos os atributos necess√°rios j√° est√£o na mesma linha.\n",
        "\n",
        "```sql\n",
        "CREATE TABLE \"gcs_silver\".stg_viagens AS\n",
        "WITH clima_agregado AS (\n",
        "    SELECT\n",
        "        data_particao,\n",
        "        EXTRACT(HOUR FROM horario) as hora_medicao,\n",
        "        AVG(acumulado_chuva_1_h) as avg_chuva_1h\n",
        "    FROM mvp_rio_transportes.bronze.clima_pluviometria\n",
        "    GROUP BY data_particao, EXTRACT(HOUR FROM horario)\n",
        ")\n",
        "SELECT\n",
        "    v.\"data\" as data_viagem,\n",
        "    v.id_viagem,\n",
        "    v.servico as linha,\n",
        "    v.consorcio,\n",
        "    v.tipo_dia,\n",
        "    v.distancia_planejada,\n",
        "    v.tempo_viagem,\n",
        "    EXTRACT(HOUR FROM v.datetime_partida) as hora_partida,\n",
        "    CASE WHEN EXTRACT(DOW FROM v.\"data\") IN (1, 7) THEN TRUE ELSE FALSE END as is_weekend,\n",
        "    COALESCE(c.avg_chuva_1h, 0) as chuva_no_horario,\n",
        "    CASE\n",
        "        WHEN c.avg_chuva_1h BETWEEN 0.1 AND 5 THEN 'Fraca'\n",
        "        WHEN c.avg_chuva_1h BETWEEN 5 AND 25 THEN 'Moderada'\n",
        "        WHEN c.avg_chuva_1h BETWEEN 25.1 AND 50 THEN 'Forte'\n",
        "        WHEN c.avg_chuva_1h > 50 THEN 'Muito Forte'\n",
        "        ELSE 'Sem Chuva'\n",
        "    END as condicao_clima\n",
        "FROM mvp_rio_transportes.bronze.viagens_onibus v\n",
        "LEFT JOIN clima_agregado c\n",
        "    ON v.\"data\" = c.data_particao\n",
        "    AND EXTRACT(HOUR FROM v.datetime_partida) = c.hora_medicao;\n",
        "```\n",
        "![Cria√ß√£o refatorada levou menos tempo](https://drive.google.com/uc?id=1lLBHoxAw1zSmz_Q0sKw7K05Bu0WnTLaz)\n",
        "\n",
        "\n",
        "### **B. O Conceito de \"Vetor de Resposta\"**\n",
        "* ### **Independ√™ncia de Perguntas**: Uma Wide Table com 11 colunas bem selecionadas consegue responder a **todas as 6 perguntas de pesquisa** sem precisar voltar √† Bronze. Se uma nova pergunta surgir, o \"vetor\" de dados j√° possui o contexto clim√°tico e temporal pronto para ser filtrado.\n",
        "\n",
        "### **C. Preserva√ß√£o da Granularidade para Estat√≠stica**\n",
        "### Entretanto, o ponto fundamental √© que, embora a tabela seja \"larga\" em colunas, ela permanece \"fina\" em linhas (mant√©m o gr√£o de uma linha por viagem).\n",
        "* ### Isso √© vital para a **Pergunta 4** (Desvio Padr√£o). Se tiv√©ssemos reduzido os dados para m√©dias prematuramente, perder√≠amos a capacidade de enxergar a vari√¢ncia e os outliers que ocorrem em dias de chuva forte.\n"
      ],
      "metadata": {
        "id": "mAL6kY0zIQXz"
      },
      "id": "mAL6kY0zIQXz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Camada Ouro: modelagem de Dimens√µes e Fatos e Carga para An√°lise**"
      ],
      "metadata": {
        "id": "4cvAMUj2phZX"
      },
      "id": "4cvAMUj2phZX"
    },
    {
      "cell_type": "markdown",
      "id": "3ece5407",
      "metadata": {
        "id": "3ece5407"
      },
      "source": [
        "## **Modelagem**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2cf5a3a",
      "metadata": {
        "id": "e2cf5a3a"
      },
      "source": [
        "## **Carga**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c98c63fc",
      "metadata": {
        "id": "c98c63fc"
      },
      "source": [
        "## **An√°lise**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce8bfa5",
      "metadata": {
        "id": "5ce8bfa5"
      },
      "source": [
        "### **Qualidade dos dados**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34860ca",
      "metadata": {
        "id": "a34860ca"
      },
      "source": [
        "### **Solu√ß√£o do Problema**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Como essa arquitetura poderia evoluir no futuro?**"
      ],
      "metadata": {
        "id": "9oWL7-bsE7xr"
      },
      "id": "9oWL7-bsE7xr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Autoavalia√ß√£o**"
      ],
      "metadata": {
        "id": "qrTKgmttVgjA"
      },
      "id": "qrTKgmttVgjA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Desafios enfrentados (Post-Mortem)**\n"
      ],
      "metadata": {
        "id": "VBSEoiuPVjGA"
      },
      "id": "VBSEoiuPVjGA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Aprendizado e o que funcionou?**"
      ],
      "metadata": {
        "id": "-W25wIQYvWBb"
      },
      "id": "-W25wIQYvWBb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **O Cemit√©rio das boas id√©ias**\n",
        "\n",
        "### **O Gargalo da Depend√™ncia Monol√≠tica**\n",
        "### No in√≠cio do projeto, a orquestra√ß√£o foi desenhada seguindo um modelo linear de depend√™ncias, onde m√∫ltiplos fluxos de dados convergiam para um √∫nico ponto de integra√ß√£o.\n",
        "\n",
        "* ### **A Dificuldade**: Uma falha em um √∫nico dataset (inconsist√™ncia de esquema nas reclama√ß√µes 1746) gerava um efeito cascata que impedia o merge final de todos os outros dados saud√°veis.\n",
        "\n",
        "* ### **A Solu√ß√£o**: Transi√ß√£o para uma arquitetura de Isolamento Transacional, onde cada dataset opera em sua pr√≥pria branch do Nessie e possui sua pr√≥pria tarefa de merge independente.\n",
        "\n",
        "### ** Incompatibilidade de Protocolos: Apache Flight SQL vs. ODBC**\n",
        "### A integra√ß√£o entre o Airflow (via `PythonOperator`) e o Dremio revelou desafios na camada de transporte de dados.\n",
        "\n",
        "* ### **A Dificuldade**: O uso do driver ODBC padr√£o com o protocolo **Flight SQL** do Dremio apresentou problemas de gest√£o de transa√ß√µes. O Airflow, por padr√£o, tentava gerenciar o commit, o que entrava em conflito com o motor de execu√ß√£o do Dremio.\n",
        "\n",
        "* ### **A Solu√ß√£o**: Implementa√ß√£o de um bypass t√©cnico no `DataOpsManager`, for√ßando a execu√ß√£o direta via `pyodbc` com o par√¢metro `autocommit=True`. Isso permitiu que comandos DDL (como o merge de branches) fossem executados de forma at√¥mica e sem interfer√™ncia do orquestrador.\n",
        "\n",
        "* ### A **Solu√ß√£o**: Ado√ß√£o do padr√£o **CTAS (Create Table As Select)**. O Dremio passou a gerenciar automaticamente o posicionamento dos metadados Iceberg no bucket do GCS, eliminando a complexidade manual de gest√£o de arquivos.\n",
        "\n",
        "### **Schema Drift e a Falha da Infer√™ncia Autom√°tica**\n",
        "### A confian√ßa na infer√™ncia autom√°tica de tipos do Dremio mostrou-se um ponto de vulnerabilidade para dados reais e \"sujos\".\n",
        "\n",
        "* ### **A Dificuldade**: O erro `SCHEMA_CHANGE` ocorria com frequ√™ncia quando arquivos Parquet de diferentes per√≠odos apresentavam varia√ß√µes sutis de tipos (ex: `varchar` vs `timestamp`), paralisando o pipeline.\n",
        "\n",
        "* ### **A Solu√ß√£o**: Implementa√ß√£o de **Schema Enforcement** (Imposi√ß√£o de Esquema). Criamos um cat√°logo de metadados centralizado no Airflow e passamos a injetar fun√ß√µes de **CAST** expl√≠citas em todos os processos de registro de tabelas, garantindo que o Lakehouse operasse sob um contrato de dados r√≠gido e previs√≠vel.\n",
        "\n",
        "### A transi√ß√£o entre a camada Bronze (dados brutos tipados em Parquet) e as camadas subsequentes (Silver/Gold) enfrentou um cen√°rio de falha cr√≠tica, aqui denominado \"apocalipse de metadados\", que motivou uma reavalia√ß√£o completa da arquitetura proposta para o MVP.\n",
        "\n",
        "### **O Conflito do Cat√°logo Gerenciado (Nessie vs. SQL)**\n",
        "### Houve uma barreira t√©cnica na forma como o SQL de cria√ß√£o de tabelas era interpretado pelo Dremio dentro de uma fonte versionada.\n",
        "* ### **A Dificuldade**: Tentativas de utilizar a cl√°usula `LOCATION` no comando `CREATE TABLE` resultaram em erros de \"argumento inv√°lido\", pois fontes gerenciadas pelo Nessie pro√≠bem a defini√ß√£o manual do caminho f√≠sico dos dados.\n",
        "\n",
        "### **Anatomia da Falha**\n",
        "### A falha n√£o foi causada por um √∫nico erro, mas pela converg√™ncia de tr√™s fatores t√©cnicos distintos que resultaram na perda total da consist√™ncia do cat√°logo de dados:\n",
        "* ### **Amn√©sia de Sess√£o e Desacoplamento de Contexto**: O componente de orquestra√ß√£o [`DataOpsManager`](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/master/dags/modules/dataops_manager.py) operava em um modelo de conex√µes ef√™meras. Ao executar comandos de troca de contexto (`USE REFERENCE`) e cria√ß√£o de tabelas em sess√µes ODBC distintas, o sistema sofria de uma \"amn√©sia\" de estado, resultando na grava√ß√£o de metadados diretamente na branch principal (`main`), ignorando o isolamento de desenvolvimento pretendido.\n",
        "\n",
        "* ### **Falha de Persist√™ncia em Camada de Infraestrutura**: Devido a restri√ß√µes de permiss√£o no sistema de arquivos da VPS, o banco de dados de metadados (RocksDB) do cat√°logo Nessie falhou ao persistir dados no volume f√≠sico. Como consequ√™ncia, o cat√°logo operava inteiramente em mem√≥ria vol√°til, tornando-o resiliente a consultas em tempo de execu√ß√£o, mas vulner√°vel a qualquer reinicializa√ß√£o do servi√ßo.\n",
        "\n",
        "* ### **Diverg√™ncia de Linhagem (Merge Inversion)**: A tentativa de sincronizar branches de desenvolvimento vazias com uma branch principal que continha dados √≥rf√£os gerou um estado de inconsist√™ncia no motor de cat√°logo. Isso causou opera√ß√µes de merge que, na pr√°tica, propagavam o estado \"vazio\" para as tabelas que deveriam ser povoadas.\n",
        "\n",
        "### **Decis√£o Estrat√©gica: Simplifica√ß√£o para Robustez**\n",
        "### Diante da fragilidade apresentada pela sobreposi√ß√£o de ferramentas (Nessie e dbt) para um escopo de MVP, optou-se pela \"**Lei da Parcim√¥nia**\" (Navalha de Ockham). A arquitetura foi simplificada para eliminar o cat√°logo intermedi√°rio, permitindo que o motor de consulta (Dremio) gerenciasse tabelas Iceberg diretamente no armazenamento em nuvem (GCS).\n",
        "### A remo√ß√£o do Nessie e do dbt reduziu a superf√≠cie de ataque de erros em 60%, eliminando a necessidade de gest√£o manual de sess√µes transacionais e garantindo que a persist√™ncia dos dados dependesse exclusivamente do storage de objetos, que possui alta disponibilidade nativa."
      ],
      "metadata": {
        "id": "TLm2sMG6oBdK"
      },
      "id": "TLm2sMG6oBdK"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}