{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielsimas/rio-viagens-onibus-analytics/blob/master/ARTIGO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**IMPORTANTE**: ***Executar esse trecho de c√≥digo para baixar o reposit√≥rio e alimentar as imagens desse arquivo.***"
      ],
      "metadata": {
        "id": "3kv79LrcVKAx"
      },
      "id": "3kv79LrcVKAx"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gabrielsimas/rio-viagens-onibus-analytics.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW1LYp76VFw4",
        "outputId": "25fe3772-e0bc-4e8c-979a-76311627bd38"
      },
      "id": "XW1LYp76VFw4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rio-viagens-onibus-analytics'...\n",
            "remote: Enumerating objects: 258, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 258 (delta 53), reused 33 (delta 16), pack-reused 172 (from 1)\u001b[K\n",
            "Receiving objects: 100% (258/258), 78.71 KiB | 4.63 MiB/s, done.\n",
            "Resolving deltas: 100% (149/149), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**IMPORTANTE**: ***Executar esse trecho de c√≥digo para baixar o reposit√≥rio e alimentar as imagens desse arquivo.***"
      ],
      "metadata": {
        "id": "3kv79LrcVKAx"
      },
      "id": "3kv79LrcVKAx"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gabrielsimas/rio-viagens-onibus-analytics.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW1LYp76VFw4",
        "outputId": "25fe3772-e0bc-4e8c-979a-76311627bd38"
      },
      "id": "XW1LYp76VFw4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rio-viagens-onibus-analytics'...\n",
            "remote: Enumerating objects: 258, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 258 (delta 53), reused 33 (delta 16), pack-reused 172 (from 1)\u001b[K\n",
            "Receiving objects: 100% (258/258), 78.71 KiB | 4.63 MiB/s, done.\n",
            "Resolving deltas: 100% (149/149), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af5e85ad",
      "metadata": {
        "id": "af5e85ad"
      },
      "source": [
        "# üöå MVP Engenharia de Dados - O Rio de Janeiro e os √înibus: Entendendo o Caos!\n",
        "## ***Nome:*** **Lu√≠s Gabriel Nascimento Simas**\n",
        "## ***Matr√≠cula:*** **4052025000943**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c17a22a",
      "metadata": {
        "id": "0c17a22a"
      },
      "source": [
        "## **Objetivo**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911589b2",
      "metadata": {
        "id": "911589b2"
      },
      "source": [
        "### O Rio de Janeiro, mundialmente conhecido por suas belezas naturais, enfrenta um desafio urbano de propor√ß√µes continentais: a mobilidade p√∫blica. O sistema de transporte rodovi√°rio, vital para o funcionamento da metr√≥pole, √© frequentemente associado √† palavra \"**Caos**\".\n",
        "### A popula√ß√£o carioca convive diariamente com incertezas: tempos de espera imprevis√≠veis, frota envelhecida, desconforto t√©rmico em uma cidade tropical e falhas na cobertura do servi√ßo, especialmente em hor√°rios noturnos. Embora iniciativas como o BRT (*Bus Rapid Transit*) e o BRS (*Bus Rapid System*) tenham sido implementadas para mitigar a lentid√£o, a percep√ß√£o de qualidade do servi√ßo permanece aqu√©m do ideal.\n",
        "### N√£o se trata apenas de desconforto, mas de um problema sist√™mico que envolve a gest√£o de cons√≥rcios privados, a fiscaliza√ß√£o do poder p√∫blico e o impacto direto na qualidade de vida e produtividade do cidad√£o.\n",
        "### Se por um lado o cen√°rio f√≠sico √© ca√≥tico, o cen√°rio digital oferece uma oportunidade de organiza√ß√£o e entendimento. A Prefeitura da Cidade do Rio de Janeiro (PCRJ), atrav√©s do portal *[data.rio](https://data.rio)*, disponibiliza um vasto reposit√≥rio de dados abertos em v√°rios formatos e alguns at√© como tabelas no **Google** *BigQuery*.\n",
        "### Esses dados incluem registros detalhados de viagens (baseados em bilhetagem e telemetria consolidada), cadastro t√©cnico da frota, dados meteorol√≥gicos do Alerta Rio e registros de reclama√ß√µes da central 1746."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3896e5c3",
      "metadata": {
        "id": "3896e5c3"
      },
      "source": [
        "### Para guiar o desenvolvimento do pipeline de dados, definimos as seguintes quest√µes-chave que esse trabalho deseja responder."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2672dc5",
      "metadata": {
        "id": "d2672dc5"
      },
      "source": [
        "\n",
        "- ### **Din√¢mica de Viagens e Demanda**\n",
        "    - #### 1. Em quais dias e faixas hor√°rias o volume de viagens realizadas atinge seu pico?\n",
        "    - #### 2. Existe uma redu√ß√£o sens√≠vel na oferta de viagens nos finais de semana comparados aos dias √∫teis?\n",
        "    - #### 3. Qual √© a dura√ß√£o m√©dia das viagens por Cons√≥rcio? Existe discrep√¢ncia significativa entre eles?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42967d32",
      "metadata": {
        "id": "42967d32"
      },
      "source": [
        "- ### **Consist√™ncia Operacional**\n",
        "    - ### 4. O tempo de viagem nas principais linhas √© consistente ou apresenta alto desvio padr√£o (imprevisibilidade)?\n",
        "    - ### 5. Em dias classificados como \"Chuvosos\" (precipita√ß√£o m√©dia acima de um limiar), observa-se aumento no tempo m√©dio de deslocamento?\n",
        "    - ### 6. A chuva influencia na quantidade total de viagens realizadas (redu√ß√£o da oferta ou demanda)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aec15831",
      "metadata": {
        "id": "aec15831"
      },
      "source": [
        "- ### Perfil da frota e qualidade\n",
        "    - ### 7. Qual √© a idade m√©dia da frota de √¥nibus ativa no Rio de Janeiro?\n",
        "    - ### 8. Quais cons√≥rcios operam com a frota mais moderna e quais possuem a frota mais obsoleta?\n",
        "    - ### 9. Existe correla√ß√£o entre a idade da frota e a presen√ßa de ar-condicionado funcional?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2bcda6b",
      "metadata": {
        "id": "f2bcda6b"
      },
      "source": [
        "- ### Percep√ß√£o do Usu√°rio (1746)\n",
        "    - ### 10. Quais s√£o as categorias de reclama√ß√£o mais frequentes relacionadas ao transporte rodovi√°rio?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be726d7",
      "metadata": {
        "id": "5be726d7"
      },
      "source": [
        "### **Escopo temporal**\n",
        "### A an√°lise compreender√° o per√≠odo de **01/01/2024** a **19/12/2025**, permitindo uma vis√£o longitudinal que abrange sazonalidades, dias √∫teis, feriados e varia√ß√µes clim√°ticas ao longo de quase dois anos completos.\n",
        "### Antes de come√ßar a explicar o mecanismo de coleta de dados que foi utilizado, √© importante mostrar em qual arquitetura nosso trabalho est√° sendo desenvolvido/implementado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af9bd61",
      "metadata": {
        "id": "2af9bd61"
      },
      "source": [
        "## üèõÔ∏è **Arquitetura da Solu√ß√£o**: Data Lakehouse H√≠brido (VPS + GCP)\n",
        "\n",
        "### Esta se√ß√£o detalha os componentes tecnol√≥gicos do MVP. A arquitetura adota uma estrat√©gia **H√≠brida**, combinando a otimiza√ß√£o de recursos de um VPS (Virtual Private Server) com a escalabilidade de armazenamento da Nuvem P√∫blica (Google Cloud).\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Provisionamento de Infraestrutura (IaC)**\n",
        "\n",
        "### A infraestrutura na nuvem n√£o foi criada manualmente. Todo o ambiente (Buckets, Permiss√µes, APIs) foi definido via **Terraform**, garantindo que o ambiente seja reprodut√≠vel, audit√°vel e imune a erros humanos.\n",
        "\n",
        "### üèóÔ∏è **Terraform Workflow**\n",
        "### **Papel:** Provisionamento de Nuvem.\n",
        "* ### **Fun√ß√£o:** Cria e gerencia os recursos na GCP (Landing, Bronze, Silver, Gold) de forma declarativa.\n",
        "* ### **Ciclo de Vida:** O deploy seguiu as fases de *Bootstrap*, *Validation*, *Plan* e *Apply*.\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>\n",
        "<strong>\n",
        "Clique para expandir: Evid√™ncias da Execu√ß√£o do Terraform\n",
        "</strong>\n",
        "</summary>\n",
        "<br>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">1. Prepara√ß√£o do Ambiente (Bootstrap)</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1Iq7M2xjMHbjlrEMHYy36VCb8tu1WXJQZ\" width=\"100%\" alt=\"Script Inicial\">\n",
        "<p><em>Setup inicial das vari√°veis e autentica√ß√£o no Google Cloud Shell.</em></p>\n",
        "<hr>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">2. Valida√ß√£o e Planejamento (Plan)</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1tdGPpca2xtP4oKW0zmfELg5opni451Ao\" width=\"100%\" alt=\"Terraform Validate\">\n",
        "<p><em>Validando a sintaxe dos arquivos .tf para garantir qualidade do c√≥digo.</em></p>\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1PIAl4zRuyEZYhEJJVuPNaItXQIO99RUm\" width=\"100%\" alt=\"Terraform Plan\">\n",
        "<p><em>Terraform Plan: O sistema calcula o \"estado futuro\" e prev√™ as mudan√ßas antes de tocar na nuvem.</em></p>\n",
        "<hr>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">3. Aplica√ß√£o e Resultado (Apply)</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1faj6oTVGA2jjHIyjzZ5EzMl1kRyUxE8S\" width=\"100%\" alt=\"Terraform Apply Sucesso\">\n",
        "<p><em>Terraform Apply: Cria√ß√£o automatizada dos recursos (Buckets, Service Accounts).</em></p>\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1JaqcI2S5XlZiu6WKHt6Ibn3nzoiGHKzK\">\n",
        "<p><em>Resultado Final: O ambiente pronto no console da Google Cloud, refletindo exatamente o c√≥digo.</em></p>\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Servidor de Processamento (Compute On-Premise)**\n",
        "\n",
        "###Para viabilizar o uso de ferramentas corporativas (Dremio/Java) em um hardware limitado, foi realizado um trabalho avan√ßado de **Engenharia de Sistemas** no VPS.\n",
        "\n",
        "### üñ•Ô∏è **Especifica√ß√µes e Otimiza√ß√£o**\n",
        "* ### **Hardware:** VPS com processador AMD EPYC (4 vCPUs) e **6GB de RAM**.\n",
        "* ### **Estrat√©gia de Swap:** Devido √† limita√ß√£o de mem√≥ria f√≠sica, configurou-se uma √°rea de swap de **22GB+** via terminal Linux. Isso permite que o *Heap* do Java (Dremio) e os processos do Airflow rodem simultaneamente sem causar *Out Of Memory (OOM)*.\n",
        "\n",
        "### üêß **Hardening e Seguran√ßa**\n",
        "### O ambiente segue o princ√≠pio do menor privil√©gio, evitando a execu√ß√£o de containers como `root` e utilizando chaves SSH para acesso.\n",
        "\n",
        "<details>\n",
        "<summary><strong>üé• Clique para expandir: Engenharia de Sistemas (Linux Tuning)</strong></summary>\n",
        "<br>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">1. Acesso Seguro e Cria√ß√£o de Usu√°rio</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1KVESwnVCpXmFbDKCJSFuEQzm7-cp6eB4\" width=\"100%\" alt=\"Acesso SSH\">\n",
        "<p><em>Acesso remoto seguro via SSH com chaves criptografadas (sem senha).</em></p>\n",
        "<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1-ruL8NLRbhD9FZ333c7H0A2sVInfK411\" width=\"100%\" alt=\"User Setup\">\n",
        "<p><em>Seguran√ßa: Cria√ß√£o de usu√°rio dedicado para evitar execu√ß√£o de containers como root.</em></p>\n",
        "<hr>\n",
        "\n",
        "<h2 style=\"margin-bottom:0\">2. Expans√£o de Mem√≥ria (Swap Strategy)</h2>\n",
        "<img src=\"https://drive.google.com/uc?id=1jy_hFiKoHqKumGM-0ZMaYvaQ8GZdCEdN\" width=\"100%\" alt=\"Swap Creation\">\n",
        "<p><em>Engenharia: Cria√ß√£o manual de arquivo de Swap e ajuste de swappiness para suportar carga pesada.</em></p>\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Orquestra√ß√£o e Ingest√£o**\n",
        "\n",
        "### üê≥ **Docker Compose**\n",
        "### **Papel:** Orquestra√ß√£o de Containers.\n",
        "* ### **Fun√ß√£o:** Sobe e conecta os servi√ßos do Airflow, Dremio, Nessie e Postgres em uma rede interna isolada, garantindo portabilidade.\n",
        "\n",
        "Aqui, temos uma imagem de nosso ambiente com os containers utilizados nessa solu√ß√£o operacionais, prints das aplica√ß√µes Lazydocker e Portainer respectivamente.\n",
        "\n",
        "![Containers em p√© com LazyDocker](https://drive.google.com/uc?id=1tQUFttV93jUKBNndXEnMsOfUpfpn0s_Q)\n",
        "\n",
        "![Container em p√© com Portainer](https://drive.google.com/uc?id=14HgdV4boaHVyKzcbDsesDmg0peh_aF2J)\n",
        "\n",
        "\n",
        "### üå™Ô∏è **Apache Airflow**\n",
        "### **Papel:** Orquestrador Geral (O \"Maestro\").\n",
        "* ### **Fun√ß√£o:** Gerencia todo o pipeline de dados. Monitora o Google Drive, dispara os workers de ingest√£o e aciona o **dbt** para as transforma√ß√µes.\n",
        "* ### **Otimiza√ß√£o:** Configurado com `LocalExecutor` para paralelizar tarefas nos 4 n√∫cleos da CPU.\n",
        "\n",
        "![Tela do Airflow com DAGs](https://drive.google.com/uc?id=1AYCCyarwBsubUZ0F-i33rFUd5FrVgIHD)\n",
        "\n",
        "### ü¶Ü **DuckDB (Worker)**\n",
        "### **Papel:** Processamento Leve de Ingest√£o.\n",
        "* ### **Fun√ß√£o:** Banco anal√≠tico embutido (*in-process*) que roda dentro do worker do Airflow.\n",
        "* ### **No MVP:** L√™ arquivos CSV brutos da camada Landing, limpa e converte para **Parquet** na camada Bronze. Sua efici√™ncia permite processar gigabytes de dados com pouca RAM.\n",
        "\n",
        "### ‚òÅÔ∏è **Google Cloud Storage (GCS)**\n",
        "### **Papel:** Armazenamento de Objetos (Storage Layer).\n",
        "* ### **Camadas:**\n",
        "    * ### **Landing:** Dados brutos (CSV).\n",
        "    * ### **Bronze:** Dados hist√≥ricos convertidos para Parquet.\n",
        "    * ### **Silver/Gold:** Dados de neg√≥cio no formato **Apache Iceberg**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **O Motor do Lakehouse (Engine & Transforma√ß√£o)**\n",
        "\n",
        "### üíé **Dremio**\n",
        "### **Papel:** SQL Lakehouse Engine.\n",
        "* ### **Fun√ß√£o:** Motor de consulta que permite executar SQL diretamente sobre o Data Lake.\n",
        "* ### **Performance:** Tunado com **Heap de 2.5GB** e **Direct Memory de 1GB** para operar no limite do hardware, delegando carga excedente para o Swap.\n",
        "\n",
        "### üßä **Apache Iceberg**\n",
        "### **Papel:** Formato de Tabela Aberto.\n",
        "* ### **Fun√ß√£o:** Camada de abstra√ß√£o que traz transa√ß√µes ACID e *Time Travel* para o Data Lake, permitindo updates e deletes seguros em arquivos Parquet.\n",
        "\n",
        "### üõ†Ô∏è **dbt (data build tool)**\n",
        "### **Papel:** L√≥gica de Transforma√ß√£o.\n",
        "* ### **Fun√ß√£o:** Compila as regras de neg√≥cio (SQL) e envia para o Dremio executar. Transforma dados da Bronze em tabelas Silver (limpas) e Gold (agregadas).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Governan√ßa**\n",
        "\n",
        "### ü¶ï **Project Nessie**\n",
        "### **Papel:** Cat√°logo de Dados (Git-for-Data).\n",
        "* ### **Fun√ß√£o:** Gerencia as vers√µes das tabelas Iceberg.\n",
        "* ### **Recurso:** Permite criar *Branches* de dados para testes isolados antes de fazer *Merge* na produ√ß√£o, similar ao Git para c√≥digo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3b6231",
      "metadata": {
        "id": "7b3b6231"
      },
      "source": [
        "### **O que √© a Arquitetura Moderna de Dados** ?\n",
        "### √â uma arquitetura **flex√≠vel**, **escal√°vel** e **baseada na nuvem**. Ela surge dos modelos tradicionais para suportar o volume e a velocidade dos dados atuais, integrando **Big Data**, **IA** e **governan√ßa**. Ela tem como caracter√≠sticas principais:\n",
        "- ### **Nuvem** e **Tempo Real**: Prioriza a elasticidade (AWS, Azure, GCP) e a disponibilidade imediata dos dados para tomadas de decis√£o r√°pidas. Neste MVP, vamos utilizar a GCP como nuvem principal, mas a solu√ß√£o que vamos implementar √© totalmente agn√≥stica em quest√µes de hospedagem.\n",
        "- ### **Dados como Produto**: O gerenciamento √© focado no valor que o dado gera para o neg√≥cio.\n",
        "- ### **Governan√ßa e Seguran√ßa**:  Implementadas na base (considerando LGPD), com forte gest√£o de metadados e interoperabilidade via APIs.\n",
        "### As abordagens mais populares dessa arquitetura s√£o:\n",
        "- ### **Data Lakehouse**: O melhor dos dois mundos (flexibilidade do Lake + confiabilidade do Warehouse). ***Ser√° a arquitetura que utilizaremos nesse MVP.***\n",
        "- ### **Data Mesh**: Descentraliza a propriedade dos dados por dom√≠nios de neg√≥cio.\n",
        "- ### **Modern Data Stack (MDS)**: Orquestra√ß√£o de diversas ferramentas especializadas.\n",
        "\n",
        "### Tem como principal objetivo unificar dados, an√°lises e cargas de trabalho de Intelig√™ncia Artificial de forma **eficiente, confi√°vel e escal√°vel**.\n",
        "### ***Neste MVP, vamos focar no uso do Data Lakehouse com a Arquitetura Medalh√£o***\n",
        "### Antes da se√ß√£o sobre a infraestrutura que ser√° utilizada, passaremos rapidamente sobre os significados e prop√≥sitos do **Data Lakehouse** e **Arquitetura Medalh√£o**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ca2540",
      "metadata": {
        "id": "e8ca2540"
      },
      "source": [
        "### **Data Lakehouse**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1jV69fy2AMAd4YZx0BACaQPwHQLkCJNH1\" alt=\"Girl in a jacket\" width=\"400\">\n",
        "<center />"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O **Data Lakehouse** representa uma evolu√ß√£o significativa na arquitetura de dados, surgindo para resolver a dicotomia hist√≥rica entre Data Warehouses e Data Lakes.\n",
        "### A premissa b√°sica √© simples, mas poderosa: **implementar funcionalidades de gest√£o de dados (t√≠picas de Warehouses) diretamente sobre o armazenamento de baixo custo e flex√≠vel (t√≠pico de Lakes)**.\n"
      ],
      "metadata": {
        "id": "HpUTZOVTZtl7"
      },
      "id": "HpUTZOVTZtl7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **O Problema Original (Silos de Dados)**\n",
        "Para entender o Lakehouse, precisamos olhar para o que existia antes:\n",
        "- ### **Data Warehouse (DW)**: √ìtimo para dados estruturados, SQL r√°pido e confiabilidade (ACID). **Entretanto**, √© caro, r√≠gido e p√©ssimo para lidar com dados n√£o estruturados (v√≠deo, √°udio, logs brutos) ou ML (Machine Learning).\n",
        "- ### **Data Lake**: √ìtimo para armazenamento (arquivos brutos) e flex√≠vel para Ci√™ncia de Dados. **Entretanto**, tende a virar um \"Data Swamp\" (p√¢ntano de dados) sem governan√ßa de Dados, n√£o suporta transa√ß√µes complexas e tem performance pobre para BI tradicional.\n",
        "\n",
        "### Muitas empresas mantinham os dois sistemas duplicados, criando complexidade e alto custo de manuten√ß√£o."
      ],
      "metadata": {
        "id": "SJn0qKNBZ0rj"
      },
      "id": "SJn0qKNBZ0rj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **O Conceito do Lakehouse**\n",
        "### O Data Lakehouse elimina a necessidade de mover os dados do Lake para o Warehouse para que sejam √∫teis. Ele adiciona uma camada de metadados e governan√ßa sobre os arquivos brutos armazenados no Lake."
      ],
      "metadata": {
        "id": "991cstBSalHS"
      },
      "id": "991cstBSalHS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Principais Pilares T√©cnicos:**\n",
        "- ### **Transa√ß√µes ACID**: Esta √© a grande virada de chave. O Lakehouse permite que m√∫ltiplos usu√°rios leiam e escrevam dados simultaneamente sem corromper a base. Se uma carga de dados falha no meio, nada √© salvo (atomicidade), garantindo que os dados estejam sempre consistentes.\n",
        "- ### **Schema Enforcement (Imposi√ß√£o de Esquema)**: O sistema previne que dados \"sujos\" ou fora do padr√£o sejam inseridos nas tabelas, garantindo a qualidade necess√°ria para relat√≥rios de BI.\n",
        "- ### **Formatos Abertos**: Diferente dos DWs tradicionais que usam formatos propriet√°rios (fechados), o Lakehouse armazena dados em formatos open-source como **Parquet** ou **ORC**. Isso evita o *vendor lock-in* (ficar preso a um fornecedor).\n",
        "- ### **Suporte Unificado (BI + IA)**: Cientistas de dados podem acessar os arquivos diretamente para treinar modelos de IA, enquanto Analistas de BI podem rodar queries SQL na mesma fonte de dados, sem duplica√ß√£o."
      ],
      "metadata": {
        "id": "Spk9o_GHaud5"
      },
      "id": "Spk9o_GHaud5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A Camada \"M√°gica\": ***Table Formats*****\n",
        "\n",
        "### Como voc√™ transforma um monte de arquivos soltos num bucket  qualquer (nuvem) em uma tabela confi√°vel com suporte a transa√ß√µes? **Atrav√©s dos Table Formats** (Formatos de Tabela).\n",
        "\n",
        "### Existem tr√™s tecnologias principais que habilitam o Lakehouse hoje:\n",
        "\n",
        "1. ### **Delta Lake** (Criado pela **Databricks**, agora *Linux Foundation*).\n",
        "2. ### **Apache Iceberg** (Criado pela **Netflix**) (**Que ser√° utilizado neste MVP nas camadas Prata e Ouro**).\n",
        "3. ### **Apache Hudi** (Criado pela Uber).\n",
        "\n",
        "### Essas tecnologias criam uma camada de log de transa√ß√µes. Quando voc√™ faz um UPDATE ou DELETE, o sistema n√£o altera o arquivo original imediatamente; ele registra a mudan√ßa no log e gerencia vers√µes dos arquivos, permitindo at√© mesmo \"viajar no tempo\" (Time Travel) para ver como os dados estavam ontem."
      ],
      "metadata": {
        "id": "m9YoSXxsbgb2"
      },
      "id": "m9YoSXxsbgb2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Arquitetura Medalh√£o** (**Medallion Architecture**)\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1tgFGNhwxYVR6LeTWLzl9pz7mObe8UW6B\" alt=\"Arquitetura Medalh√£o\" width=\"700\">\n",
        "<center />"
      ],
      "metadata": {
        "id": "-_-WRu5_cvST"
      },
      "id": "-_-WRu5_cvST"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### √â muito comum organizar os dados dentro de um Lakehouse em tr√™s camadas l√≥gicas para garantir a qualidade:\n",
        "\n",
        "1.\t### **Bronze (Raw)**: Dados brutos, exatamente como chegaram da fonte. Nenhuma limpeza √© feita aqui.\n",
        "2.\t### **Silver (Refined)**: Dados limpos, filtrados e com tipos corrigidos. √â a \"verdade √∫nica\" da organiza√ß√£o.\n",
        "3.\t### **Gold (Curated)**: Dados agregados, modelados (ex: Star Schema) e prontos para consumo em dashboards e relat√≥rios executivos.\n",
        "\n",
        "### Al√©m disso, n√£o vamos utilizar uma stack propriet√°ria, vamos utilizar um conceito bem atual chamado **Modern Data Stack in a Box**, onde mostraremos quais ferramentas vamos usar em nossa solu√ß√£o."
      ],
      "metadata": {
        "id": "hG6_BxncdxtU"
      },
      "id": "hG6_BxncdxtU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **O Conceito: \"Modern Data Stack in a Box\"**\n",
        "### Refere-se √† implanta√ß√£o de um ambiente completo de an√°lise de dados em uma √∫nica m√°quina (como um laptop ou servidor robusto). Essa abordagem utiliza ferramentas open-source eficientes, eliminando a complexidade e o custo de Data Warehouses em nuvem para muitos casos de uso.\n",
        "### **Componentes Principais:**\n",
        "### A arquitetura replica as camadas de uma stack moderna tradicional, mas otimizada para execu√ß√£o local:\n",
        "- ### **Ingest√£o (EL)**: Ferramentas como *Airflow* ou *Airbyte* para extrair e carregar dados.\n",
        "- ### **Armazenamento (OLAP)**: Uso de bancos de dados anal√≠ticos embutidos de alta performance, sendo o *DuckDB* o principal protagonista.\n",
        "- ### **Transforma√ß√£o (T)**: Uso do *dbt* para limpeza e modelagem via *SQL* com controle de vers√£o.\n",
        "- ### **BI & Analytics**: Visualiza√ß√£o com *Apache Superset*, *Metabase* ou *Streamlit*.\n",
        "\n",
        "### **Benef√≠cios**\n",
        "- ### **Efici√™ncia de Custo**: Elimina gastos recorrentes com infraestrutura de nuvem.\n",
        "- ### **Produtividade**: Simplifica o desenvolvimento e permite Provas de Conceito (PoCs) r√°pidas sem configurar sistemas distribu√≠dos.\n",
        "- ### **Performance**: Tira proveito do hardware moderno local para processamento r√°pido.\n"
      ],
      "metadata": {
        "id": "q8DSZh3IeMxQ"
      },
      "id": "q8DSZh3IeMxQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Trade-off** (**O contraponto**)\n",
        "### Essa abordagem oferece controle total e portabilidade. **Entretanto**, a principal concess√£o √© a perda da \"escalabilidade infinita\" das plataformas de nuvem (como *Snowflake* ou *BigQuery*), j√° que o sistema est√° limitado ao hardware da m√°quina √∫nica, o que pode ser um gargalo para volumes massivos de dados ou alta concorr√™ncia de usu√°rios. Para o nosso trabalho, a estrutura que temos vai funcionar muito bem, e na se√ß√£o Conclus√£o em ***‚ÄúO que ser√° feito nessa solu√ß√£o para um futuro escal√°vel e incerto?‚Äù*** explicaremos como podemos melhorar essa infraestrutura para escala e otimiza√ß√£o futura quando os dados crescerem demais."
      ],
      "metadata": {
        "id": "IqEPHa6pfzoC"
      },
      "id": "IqEPHa6pfzoC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sobre o uso da Arquitetura neste MVP**\n",
        "### A arquitetura segue o paradigma de *Data Lakehouse*, unificando a flexibilidade de armazenamento de baixo custo (*Google Cloud Storage*) com a performance de consulta SQL (*Dremio*), sem a necessidade de um Data Warehouse propriet√°rio caro.\n",
        "### A solu√ß√£o foi desenhada para ser *agn√≥stica de nuvem* na camada de computa√ß√£o, rodando inteiramente em *cont√™ineres Docker*, o que garante portabilidade e reprodutibilidade.\"\n",
        "### A arquitetura utilizada nesse MVP eleva o n√≠vel de *\"Modern Data Stack\"* implementando um padr√£o avan√ßad√≠ssimo conhecido como **WAP** (*Write-Audit-Publish* ou *Escrever-Auditar-Publicar*), trazendo a sem√¢ntica do *Git* (*branches*, *commits*, *merges*) para a engenharia de dados.\n",
        "### O uso do *Project Nessie* como cat√°logo para gerenciar tabelas *Iceberg* √© o grande diferencial aqui. Isso resolve um dos maiores pesadelos da engenharia de dados: a inconsist√™ncia de dados durante o processamento.\n",
        "\n",
        "### Chega de conversa, vamos pra parte pr√°tica: nosso MVP funcionando propriamente dito."
      ],
      "metadata": {
        "id": "2xbyARcngNq3"
      },
      "id": "2xbyARcngNq3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=16Tbkf1vEDtZJaxeBMrT7U6XAQv7cykey\" alt=\"Talk is cheap, show me the code\" width=\"500\">\n",
        "<center />"
      ],
      "metadata": {
        "id": "Cb55z8_iMEjF"
      },
      "id": "Cb55z8_iMEjF"
    },
    {
      "cell_type": "markdown",
      "id": "e2289d76",
      "metadata": {
        "id": "e2289d76"
      },
      "source": [
        "## **Base de Dados alvo**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Como dissemos anteriormente, n√£o utilizamos apenas uma base de dados, mas multiplas de dentro do site dados.rio atrav√©s do Google BigQuery. Abaixo, mostramos o cat√°logo de metadados baseado nas informa√ß√µes do pr√≥prio provedor dos dados: A Prefeitura da Cidade do Rio de Janeiro.\n",
        "### **Disclaimer**: ***Infelizmente o tamanho da fonte nas tabelas ficou pequeno por limita√ß√µes do pr√≥prio Colab.***"
      ],
      "metadata": {
        "id": "OGFOtqikCdwj"
      },
      "id": "OGFOtqikCdwj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cat√°logo de Dados**\n"
      ],
      "metadata": {
        "id": "jVhSAY-tOCMV"
      },
      "id": "jVhSAY-tOCMV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Dataset**: *adm_central_atendimento_1746*\n",
        "### **Tabela**: *chamados*\n",
        "### **Descri√ß√£o**: Essa tabela det√©m todos os dados de reclama√ß√µes da Central 1746 da Cidade do Rio de Janeiro, mas os dados dessa tabela s√£o apenas sobre transportes de uma maneira geral. Aqui n√£o conseguimos separar por empresa, cons√≥rcio ou dados que identifiquem o alvo das reclama√ß√µes.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **id_unidade_organizacional** | STRING | NULLABLE | Identificador √∫nico, no banco de dados, do √≥rg√£o que executa o chamado. Por exemplo: identificador da COMLURB quando o chamado √© relativo a limpeza urbana. |\n",
        "| **nome_unidade_organizacional** | STRING | NULLABLE | Nome do √≥rg√£o que executa a demanda. Por exemplo: COMLURB quando a demanda √© relativa a limpeza urbana. |\n",
        "| **id_unidade_organizacional_mae** | STRING | NULLABLE | ID da unidade organizacional m√£e do org√£o que executa a demanda. Por exemplo: \"CVA - Coordena√ß√£o de Vigil√¢ncia de Alimentos\" √© quem executa a demanda e obede a unidade organizacional m√£e \"IVISA-RIO - Instituto Municipal de Vigil√¢ncia Sanit√°ria, de Zoonoses e de Inspe√ß√£o Agropecu√°ria\". A coluna se refere ao ID deste √∫ltimo. |\n",
        "| **unidade_organizacional_ouvidoria** | STRING | NULLABLE | Booleano indicando se o chamado do cidad√£o foi feita Ouvidoria ou n√£o. 1 caso sim, 0 caso n√£o. |\n",
        "| **categoria** | STRING | NULLABLE | Categoria do chamado. Exemplo: Servi√ßo, informa√ß√£o, sugest√£o, elogio, reclama√ß√£o, cr√≠tica. |\n",
        "| **id_tipo** | STRING | NULLABLE | Identificador √∫nico, no banco de dados, do tipo do chamado. Ex: Ilumina√ß√£o p√∫blica. |\n",
        "| **tipo** | STRING | NULLABLE | Nome do tipo do chamado. Ex: Ilumina√ß√£o p√∫blica. |\n",
        "| **id_subtipo** | STRING | NULLABLE | Identificador √∫nico, no banco de dados, do subtipo do chamado. Ex: Reparo de l√¢mpada apagada. |\n",
        "| **subtipo** | STRING | NULLABLE | Nome do subtipo do chamado. Ex: Reparo de l√¢mpada apagada. |\n",
        "| **status** | STRING | NULLABLE | Status do chamado. Ex. Fechado com solu√ß√£o, aberto em andamento, pendente etc. |\n",
        "| **longitude** | FLOAT | NULLABLE | Longitude do lugar do evento que motivou o chamado. |\n",
        "| **latitude** | FLOAT | NULLABLE | Latitude do lugar do evento que motivou o chamado. |\n",
        "| **data_alvo_finalizacao** | DATETIME | NULLABLE | Data prevista para o atendimento do chamado. Caso prazo_tipo seja D fica em branco at√© o diagn√≥stico ser feito. |\n",
        "| **data_alvo_diagnostico** | DATETIME | NULLABLE | Data prevista para fazer o diagn√≥stico do servi√ßo. Caso prazo_tipo seja F esta data fica em branco. |\n",
        "| **data_real_diagnostico** | DATETIME | NULLABLE | Data em que foi feito o diagn√≥stico do servi√ßo. Caso prazo_tipo seja F esta data fica em branco. |\n",
        "| **tempo_prazo** | INTEGER | NULLABLE | Prazo para o servi√ßo ser feito. Em dias ou horas ap√≥s a abertura do chamado. Caso haja diagn√≥stico o prazo conta ap√≥s se fazer o diagn√≥stico. |\n",
        "| **prazo_unidade** | STRING | NULLABLE | Unidade de tempo utilizada no prazo. Dias ou horas. D ou H. |\n",
        "| **prazo_tipo** | STRING | NULLABLE | Diagn√≥stico ou finaliza√ß√£o. D ou F. Indica se a chamada precisa de diagn√≥stico ou n√£o. Alguns servi√ßos precisam de avalia√ß√£o para serem feitos, neste caso √© feito o diagn√≥stico. Por exemplo, pode de √°rvore. H√° a necessidade de um engenheiro ambiental verificar a necessidade da poda ou n√£o. |\n",
        "| **dentro_prazo** | STRING | NULLABLE | Indica se a data alvo de finaliza√ß√£o do chamado ainda est√° dentro do prazo estipulado. |\n",
        "| **situacao** | STRING | NULLABLE | Identifica se o chamado foi encerrado. |\n",
        "| **tipo_situacao** | STRING | NULLABLE | Indica o status atual do chamado entre as categorias Atendido, Atendido parcialmente, N√£o atendido, N√£o constatado e Andamento. |\n",
        "| **justificativa_status** | STRING | NULLABLE | Justificativa que os √≥rg√£os usam ao definir o status. Exemplo: SEM POSSIBILIDADE DE ATENDIMENTO - justificativa: Fora de √°rea de atua√ß√£o do municipio. |\n",
        "| **reclamacoes** | INTEGER | NULLABLE | Quantidade de reclama√ß√µes. |\n",
        "| **extracted_at** | TIMESTAMP | NULLABLE | Data e hora em que o registro foi extra√≠do pelo Airbyte. |\n",
        "| **updated_at** | STRING | NULLABLE | Data da ultima atualiza√ß√£o. |\n",
        "| **data_particao** | DATE | NULLABLE | Data de parti√ß√£o dos dados. Trunc(data_inicio). |"
      ],
      "metadata": {
        "id": "VuRDi6OvQX6Q"
      },
      "id": "VuRDi6OvQX6Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**: *clima_pluviometro*\n",
        "### **Tabela**: *estacoes_alertario*\n",
        "### **Descri√ß√£o**: Dados das unidades meteorol√≥gicas com seus respectivos dados, serve como dado mestre para a tabela de *taxa_precipitacao_alertario*.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **id_estacao** | STRING | NULLABLE | Identificador √∫nico da esta√ß√£o (meteorol√≥gica, pluviom√©trica ou de monitoramento) no banco de dados. |\n",
        "| **estacao** | STRING | NULLABLE | Nome ou designa√ß√£o comum da esta√ß√£o. Exemplo: \"Copacabana\", \"Tijuca\", \"S√£o Crist√≥v√£o\". |\n",
        "| **latitude** | FLOAT | NULLABLE | Coordenada geogr√°fica (latitude) da localiza√ß√£o da esta√ß√£o em graus decimais. |\n",
        "| **longitude** | FLOAT | NULLABLE | Coordenada geogr√°fica (longitude) da localiza√ß√£o da esta√ß√£o em graus decimais. |\n",
        "| **cota** | FLOAT | NULLABLE | Altitude da esta√ß√£o em rela√ß√£o ao n√≠vel do mar (geralmente expressa em metros). |\n",
        "| **x** | FLOAT | NULLABLE | Coordenada cartesiana X no sistema de proje√ß√£o (provavelmente UTM) para georreferenciamento plano. |\n",
        "| **y** | FLOAT | NULLABLE | Coordenada cartesiana Y no sistema de proje√ß√£o (provavelmente UTM) para georreferenciamento plano. |\n",
        "| **endereco** | STRING | NULLABLE | Endere√ßo f√≠sico, logradouro ou ponto de refer√™ncia onde a esta√ß√£o est√° instalada. |\n",
        "| **situacao** | STRING | NULLABLE | Status operacional atual da esta√ß√£o. Exemplo: \"Operante\", \"Nao operante\". |\n",
        "| **data_inicio_operacao** | DATETIME | NULLABLE | Data e hora em que a esta√ß√£o come√ßou a operar e coletar dados. |\n",
        "| **data_fim_operacao** | DATETIME | NULLABLE | Data e hora em que a esta√ß√£o encerrou suas atividades (caso tenha sido desativada). |\n",
        "| **data_atualizacao** | DATETIME | NULLABLE | Data e hora da √∫ltima atualiza√ß√£o cadastral ou sincroniza√ß√£o dos dados da esta√ß√£o. |\n"
      ],
      "metadata": {
        "id": "qWjDrbbVQeMF"
      },
      "id": "qWjDrbbVQeMF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**: *clima_pluviometro*\n",
        "\n",
        "### **Tabela**: *taxa_precipitacao_alertario*\n",
        "### **Descri√ß√£o**: Informa√ß√µes sobre o tempo na Cidade do Rio de Janeiro com informa√ß√µes de acumulados de chuva de 15 minutos, 1 hora, 4 horas, 24 horas e 96 horas.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **primary_key** | STRING | NULLABLE | Chave prim√°ria criada a partir da coluna id_estacao e da data_medicao. Serve para evitar dados duplicados. |\n",
        "| **id_estacao** | STRING | NULLABLE | Identificador √∫nico da esta√ß√£o de monitoramento que registrou a medi√ß√£o. (Chave estrangeira para a tabela de esta√ß√µes). |\n",
        "| **acumulado_chuva_15_min** | FLOAT | NULLABLE | Acumulado de chuva em 15 minutos. |\n",
        "| **acumulado_chuva_1_h** | FLOAT | NULLABLE | Acumulado de chuva em 1 hora. |\n",
        "| **acumulado_chuva_4_h** | FLOAT | NULLABLE | Acumulado de chuva em 4 horas. |\n",
        "| **acumulado_chuva_24_h** | FLOAT | NULLABLE | Acumulado de chuva em 24 horas. |\n",
        "| **acumulado_chuva_96_h** | FLOAT | NULLABLE | Acumulado de chuva em 96 horas. |\n",
        "| **horario** | TIME | NULLABLE | Hor√°rio no qual foi realizada a medi√ß√£o. |\n",
        "| **data_particao** | DATE | NULLABLE | Data em que foi realizada a medi√ß√£o. |\n"
      ],
      "metadata": {
        "id": "Cs7oL8LpR4uT"
      },
      "id": "Cs7oL8LpR4uT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**: *transporte_rodoviario_municipal*\n",
        "### **Tabela**: *licenciamento_veiculo*\n",
        "### **Descri√ß√£o**: Tabela sobre o cadastro da frota de √înibus da Cidade do Rio de Janeiro. Aqui podemos verificar a idade da frota e podemos cruzar com a tabela *viagem_onibus* onde podemos obter informa√ß√µes sobre o numero de ordem, cons√≥rcio, placa e etc.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **carroceria** | STRING | NULLABLE | Descri√ß√£o do modelo da carroceria. |\n",
        "| **id_chassi** | INTEGER | NULLABLE | C√≥digo do modelo do chassi. |\n",
        "| **id_fabricante_chassi** | INTEGER | NULLABLE | Identificador do fabricante do chassi. |\n",
        "| **nome_chassi** | STRING | NULLABLE | Descri√ß√£o do modelo do chassi. |\n",
        "| **id_planta** | INTEGER | NULLABLE | C√≥digo da planta do ve√≠culo. |\n",
        "| **tipo_veiculo** | STRING | NULLABLE | Tipo de ve√≠culo. |\n",
        "| **status** | STRING | NULLABLE | Status do ve√≠culo. |\n",
        "| **data_inicio_vinculo** | DATE | NULLABLE | Data de in√≠cio do v√≠nculo do ve√≠culo no STU. |\n",
        "| **data_ultima_vistoria** | DATE | NULLABLE | Data da √∫ltima vistoria do ve√≠culo. |\n",
        "| **ano_ultima_vistoria** | INTEGER | NULLABLE | Ano atualizado da √∫ltima vistoria realizada pelo ve√≠culo. |\n",
        "| **ultima_situacao** | STRING | NULLABLE | √öltima situa√ß√£o do ve√≠culo na data atual. |\n",
        "| **tecnologia** | STRING | NULLABLE | Tecnologia utilizada no ve√≠culo [BASICO, MIDI, MINI, PADRON, ARTICULADO]. |\n",
        "| **quantidade_lotacao_pe** | INTEGER | NULLABLE | Capacidade de passageiros em p√©. |\n",
        "| **quantidade_lotacao_sentado** | INTEGER | NULLABLE | Capacidade de passageiros sentados. |\n",
        "| **tipo_combustivel** | STRING | NULLABLE | Tipo de combust√≠vel utilizado. |\n",
        "| **indicador_ar_condicionado** | BOOLEAN | NULLABLE | Indicador se possui ar condicionado [Verdadeiro/Falso]. |\n",
        "| **indicador_elevador** | BOOLEAN | NULLABLE | Indicador se possui elevador [Verdadeiro/Falso]. |\n",
        "| **indicador_usb** | BOOLEAN | NULLABLE | Indicador se tem USB [Verdadeiro/Falso]. |\n",
        "| **indicador_wifi** | BOOLEAN | NULLABLE | Indicador se tem Wi-fi [Verdadeiro/Falso]. |\n",
        "| **indicador_veiculo_lacrado** | BOOLEAN | NULLABLE | Indicador se o ve√≠culo estava lacrado na data atual [Verdadeiro/Falso]. |\n",
        "| **indicador_data_ultima_vistoria_tratada** | BOOLEAN | NULLABLE | Indica se a data da √∫ltima vistoria original passou por algum processo de tratamento, limpeza ou normaliza√ß√£o. |\n",
        "| **data_arquivo_fonte** | DATE | NULLABLE | Data do arquivo do STU com os dados utilizados. |\n",
        "| **versao** | STRING | NULLABLE | C√≥digo de controle de vers√£o [SHA do GitHub]. |\n",
        "| **datetime_ultima_atualizacao** | DATETIME | NULLABLE | √öltima atualiza√ß√£o [GMT-3]. |\n",
        "| **id_execucao_dbt** | STRING | NULLABLE | Identificador da execu√ß√£o do DBT que modificou o dado pela √∫ltima vez. |\n",
        "| **ano_ultima_vistoria_atualizado** | INTEGER | NULLABLE | Ano correspondente √† √∫ltima vistoria, recalculado ou corrigido ap√≥s processos de tratamento de dados. |\n"
      ],
      "metadata": {
        "id": "3KD-Yl95eZYI"
      },
      "id": "3KD-Yl95eZYI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**: *transporte_rodoviario_municipal*\n",
        "### **Tabela**: *viagem_onibus*\n",
        "### **Descri√ß√£o**: Essa √© a tabela na qual temos as informa√ß√µes das viagens de todos os √¥nibus que circulam na Cidade do Rio de Janeiro.\n",
        "\n",
        "| Nome do Campo | Tipo | Modo | Descri√ß√£o |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **data** | DATE | NULLABLE | Data da viagem. |\n",
        "| **consorcio** | STRING | NULLABLE | Cons√≥rcio ao qual o servi√ßo pertence. |\n",
        "| **tipo_dia** | STRING | NULLABLE | Dia da semana - categorias: Dia √ötil, S√°bado, Domingo e Ponto Facultativo. |\n",
        "| **id_empresa** | STRING | NULLABLE | C√≥digo identificador da empresa que opera o ve√≠culo. |\n",
        "| **id_veiculo** | STRING | NULLABLE | C√≥digo identificador do ve√≠culo [n√∫mero de ordem]. |\n",
        "| **id_viagem** | STRING | NULLABLE | C√≥digo √∫nico identificador da viagem. |\n",
        "| **servico** | STRING | NULLABLE | Servi√ßo realizado pelo ve√≠culo [com base na identifica√ß√£o do trajeto]. |\n",
        "| **shape_id** | STRING | NULLABLE | C√≥digo identificador do shape [trajeto]. |\n",
        "| **sentido** | STRING | NULLABLE | Sentido da linha. |\n",
        "| **datetime_partida** | DATETIME | NULLABLE | Hor√°rio de in√≠cio da viagem. |\n",
        "| **datetime_chegada** | DATETIME | NULLABLE | Hor√°rio de fim da viagem. |\n",
        "| **tempo_viagem** | INTEGER | NULLABLE | Tempo aferido da viagem (minutos). |\n",
        "| **distancia_planejada** | FLOAT | NULLABLE | Dist√¢ncia do shape [trajeto] planejado (km). |\n",
        "| **perc_conformidade_shape** | FLOAT | NULLABLE | Percentual de sinais emitidos dentro do shape [trajeto] ao longo da viagem. |\n",
        "| **perc_conformidade_registros** | FLOAT | NULLABLE | Percentual de minutos da viagem com registro de sinal de GPS. |\n",
        "| **versao_modelo** | STRING | NULLABLE | C√≥digo de controle de vers√£o [SHA do GitHub]. |"
      ],
      "metadata": {
        "id": "QDKORFPihRqi"
      },
      "id": "QDKORFPihRqi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Essas s√£o as tabelas que ser√£o convertidas em outros conjuntos de dados para gerar os insights e responder as quest√µes presentes no *objetivo* desse trabalho."
      ],
      "metadata": {
        "id": "3c36vmyfiKkC"
      },
      "id": "3c36vmyfiKkC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coleta e Ingest√£o de Dados para a Camada Bronze**"
      ],
      "metadata": {
        "id": "4cZaAfMvGaCv"
      },
      "id": "4cZaAfMvGaCv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Origem dos Dados e Desafios Iniciais**\n",
        "Devido a restri√ß√µes de acesso via Conta de Servi√ßo ao Google BigQuery do **data.rio**, a estrat√©gia de coleta foi adaptada. Realizamos a extra√ß√£o manual das tabelas para arquivos **.csv** (limitados a 1 GB cada) e o upload para uma estrutura organizada no **Google Drive**.\n",
        "\n",
        "![Estrutura de Pastas com arquivos coletados](https://drive.google.com/uc?id=13RVLAW-ASzXjo-M8gAUSJbcO7OZtEr7W)\n",
        "\n"
      ],
      "metadata": {
        "id": "FHjsq_FsGeYj"
      },
      "id": "FHjsq_FsGeYj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O volume total coletado √© de **6,71 GB**. Embora modesto para padr√µes de Big Data, este volume √© ideal para demonstrar a efici√™ncia de ferramentas *open source* em processar dados que superam a capacidade de mem√≥ria convencional de m√°quinas locais, utilizando t√©cnicas de *Lazy Loading* e *Streaming*.\n",
        "\n",
        "![Estrutura de dentro de uma pasta](https://drive.google.com/uc?id=1irSDz-fItos9fnblISWjphs-3LyxAOpd)"
      ],
      "metadata": {
        "id": "91jXPpE_GmJ_"
      },
      "id": "91jXPpE_GmJ_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coleta e Ingest√£o de Dados para a Camada Bronze**\n",
        "\n",
        "### **Origem dos Dados e Desafios Iniciais**\n",
        "Devido a restri√ß√µes de acesso via Conta de Servi√ßo ao Google BigQuery do **data.rio**, a estrat√©gia de coleta foi adaptada. Realizamos a extra√ß√£o manual das tabelas para arquivos **.csv** (limitados a 1 GB cada) e o upload para uma estrutura organizada no **Google Drive**.\n",
        "\n",
        "![Estrutura de Pastas com arquivos coletados](https://drive.google.com/uc?id=13RVLAW-ASzXjo-M8gAUSJbcO7OZtEr7W)\n",
        "\n",
        "### O volume total coletado √© de **6,71 GB**. Embora modesto para padr√µes de Big Data, este volume √© ideal para demonstrar a efici√™ncia de ferramentas *open source* em processar dados que superam a capacidade de mem√≥ria convencional de m√°quinas locais, utilizando t√©cnicas de *Lazy Loading* e *Streaming*.\n",
        "\n",
        "![Estrutura de dentro de uma pasta](https://drive.google.com/uc?id=1irSDz-fItos9fnblISWjphs-3LyxAOpd)\n",
        "\n",
        "---\n",
        "\n",
        "### **Orquestra√ß√£o com Apache Airflow**\n",
        "### Para automatizar a movimenta√ß√£o desses dados para a nuvem, utilizamos o **Apache Airflow** rodando em uma VPS via Docker. O Airflow √© uma plataforma de c√≥digo aberto para autorar, agendar e monitorar fluxos de trabalho.\n",
        "\n",
        "* ### **O que √© uma DAG?** No Airflow, o pipeline √© definido como uma **DAG (Directed Acyclic Graph)**. Trata-se de um conjunto de tarefas organizadas de forma que reflitam suas depend√™ncias e rela√ß√µes, garantindo que o fluxo de dados siga uma dire√ß√£o l√≥gica sem loops infinitos.\n",
        "* ### **Pipeline de Ingest√£o:** Nossa DAG foi desenvolvida utilizando o padr√£o *Factory* (Orchestrator), permitindo que novos datasets sejam adicionados apenas via configura√ß√£o, garantindo escalabilidade e manutenibilidade ao projeto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Fluxo de Dados: Drive para Landing Zone**\n",
        "O processo de coleta segue um fluxo rigoroso de Engenharia de Dados, visando performance e integridade:\n",
        "\n",
        "1.  ### **Discovery:** O Airflow varre as pastas do Google Drive identificando novos arquivos `.csv` de forma din√¢mica.\n",
        "2.  ### **Streaming Ingestion:** Os dados s√£o baixados via fluxo de bits (streaming) diretamente para o disco tempor√°rio da VPS, evitando estouro de mem√≥ria RAM.\n",
        "3.  ### **Landing Zone (Raw Layer):** O arquivo bruto √© persistido no **Google Cloud Storage (GCS)** em seu formato original (CSV) para fins de auditoria e conformidade.\n",
        "4.  ### **Bronze Layer (Hive Partitioning):** Utilizando o motor **DuckDB**, realizamos a convers√£o para **Parquet** (formato colunar de alta performance) e salvamos os dados na camada Bronze utilizando o **particionamento Hive** (`ano=YYYY/mes=MM/dia=DD`).\n",
        "\n",
        "![GIF da Execu√ß√£o da DAG no Airflow](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/blob/feature%2Fcriacao-ambiente/INSIRA_AQUI_O_LINK_DO_SEU_GIF?raw=1)\n",
        "\n",
        "---\n",
        "\n",
        "### Seguran√ßa e Governan√ßa (IAM)**\n",
        "### A seguran√ßa do pipeline √© baseada no princ√≠pio do **Privil√©gio M√≠nimo** atrav√©s do **IAM (Identity and Access Management)** do Google Cloud:\n",
        "\n",
        "* ### **Service Account (SA):** Foi criada uma identidade program√°tica exclusiva para o Airflow.\n",
        "* ### **Chaves Privadas:** A autentica√ß√£o √© feita via chave JSON criptografada, armazenada de forma segura em volumes protegidos no Docker.\n",
        "* ### **Escopo de Permiss√µes:**\n",
        "    * ### **Google Drive API:** Acesso de leitura estrito √†s pastas de mobilidade do projeto.\n",
        "    * ### **GCS Storage Object Admin:** Permiss√£o para gerenciar objetos exclusivamente nos buckets `mvp-landing` e `mvp-bronze`.\n",
        "\n",
        "> ### **Nota de Arquiteto:** Nenhuma credencial sens√≠vel √© exposta no c√≥digo-fonte. Toda a configura√ß√£o de ambiente (IDs de pastas, nomes de buckets e chaves) √© injetada via vari√°veis de ambiente protegidas no container.\n",
        "\n",
        "---\n",
        "\n",
        "### **Reposit√≥rio de C√≥digo**\n",
        "### Toda a l√≥gica de orquestra√ß√£o, classes de gerenciamento de ingest√£o e defini√ß√µes das DAGs est√£o versionadas no GitHub para garantir a reprodutibilidade do experimento:\n",
        "\n",
        "> üîó [**Acessar Diret√≥rio das DAGs no GitHub**](https://github.com/gabrielsimas/rio-viagens-onibus-analytics/tree/master/dags)\n",
        "\n",
        "---\n",
        "\n",
        "### **Detalhes da execu√ß√£o**\n",
        "### Tive alguns problemas na comunica√ß√£o do Airflow com o Google drive e com isso, tive de bypassar seu componente de acesso com c√≥digo direto na classe de Ingest√£o `IngestionManager`, troquei o trecho:\n",
        "```python\n",
        "from airflow.providers.google.cloud.hooks.drive import GoogleDriveHook\n",
        "\n",
        "def __init__(self):\n",
        "    self._drive_hook = GoogleDriveHook(gcp_conn_id=\"google_cloud_default\")\n",
        "\n",
        "def list_files(self, folder_id):\n",
        "    return self._drive_hook.get_conn().files().list(q=f\"'{folder_id}' in parents\").execute()\n",
        "```\n",
        "\n",
        "### Para:\n",
        "```python\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "def __init__(self):\n",
        "    self._key_path = \"/opt/airflow/keys/sa-airflow.json\"\n",
        "    self._scopes = ['[https://www.googleapis.com/auth/drive.readonly](https://www.googleapis.com/auth/drive.readonly)']\n",
        "\n",
        "def _get_drive_service(self):\n",
        "    creds = service_account.Credentials.from_service_account_file(\n",
        "        self._key_path, scopes=self._scopes\n",
        "    )\n",
        "    return build('drive', 'v3', credentials=creds)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Para resolver, implementamos um *bypass* direto via Google Discovery API no `IngestionManager`, substituindo o Hook padr√£o por uma solu√ß√£o mais est√°vel:\n",
        "\n",
        "### **Antes (Padr√£o Airflow):**\n",
        "```python\n",
        "from airflow.providers.google.cloud.hooks.drive import GoogleDriveHook\n",
        "\n",
        "def __init__(self):\n",
        "    self._drive_hook = GoogleDriveHook(gcp_conn_id=\"google_cloud_default\")\n",
        "\n",
        "def list_files(self, folder_id):\n",
        "    return self._drive_hook.get_conn().files().list(q=f\"'{folder_id}' in parents\").execute()\n",
        "```\n",
        "\n",
        "### **Depois (Bypass Direto - IngestionManager):**\n",
        "```python\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "def __init__(self):\n",
        "    self._key_path = \"/opt/airflow/keys/sa-airflow.json\"\n",
        "    self._scopes = ['[https://www.googleapis.com/auth/drive.readonly](https://www.googleapis.com/auth/drive.readonly)']\n",
        "\n",
        "def _get_drive_service(self):\n",
        "    creds = service_account.Credentials.from_service_account_file(\n",
        "        self._key_path, scopes=self._scopes\n",
        "    )\n",
        "    return build('drive', 'v3', credentials=creds)\n",
        "```\n",
        "### **Integra√ß√£o com Dremio (DataOpsManager)**\n",
        "### Para finalizar a camada Bronze, automatizamos o registro das tabelas como Apache Iceberg, permitindo que o Dremio gerencie vers√µes de forma at√¥mica:\n",
        "```python\n",
        "def ensure_table_exists(self, dataset_name: str, branch_name: str, bucket_name: str):\n",
        "    table_path = f\"{self._catalog}.bronze.{dataset_name}\"\n",
        "    storage_location = f\"gs://{bucket_name}/{dataset_name}\"\n",
        "\n",
        "    create_sql = f\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS {table_path}\n",
        "        AT BRANCH \"{branch_name}\"\n",
        "        LOCATION '{storage_location}'\n",
        "    \"\"\"\n",
        "    self._hook.run(create_sql)\n",
        "```"
      ],
      "metadata": {
        "id": "SG-SazynHMmu"
      },
      "id": "SG-SazynHMmu"
    },
    {
      "cell_type": "markdown",
      "id": "4e4ab599",
      "metadata": {
        "id": "4e4ab599"
      },
      "source": [
        "### Ap√≥s essas altera√ß√µes, a ingest√£o ocorre de forma paralela e segura, garantindo uma base s√≥lida para a an√°lise explorat√≥ria e a cria√ß√£o das camadas Prata e Ouro via dbt. Como mostrado abaixo:\n",
        "\n",
        "### [Mostrar o v√≠deo de sucesso no Airflow]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Mostrar como ficou a pasta Landing no GCS]"
      ],
      "metadata": {
        "id": "7C95_NUKH4YO"
      },
      "id": "7C95_NUKH4YO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Mostrar como ficou a pasta bronze dentro do GCS]"
      ],
      "metadata": {
        "id": "puyemU6MINbE"
      },
      "id": "puyemU6MINbE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Mostrar como ficou a estrutura criada no Dremio]"
      ],
      "metadata": {
        "id": "ApzsGY5FIZAK"
      },
      "id": "ApzsGY5FIZAK"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vn65aSa6Idkx"
      },
      "id": "vn65aSa6Idkx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Camada Prata: tratamento dos dados, enriquecimento, cria√ß√£o de Esquema inicial e Carga**"
      ],
      "metadata": {
        "id": "ia6OA5tnmonl"
      },
      "id": "ia6OA5tnmonl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Camada Ouro: modelagem de Dimens√µes e Fatos e Carga para An√°lise**"
      ],
      "metadata": {
        "id": "4cvAMUj2phZX"
      },
      "id": "4cvAMUj2phZX"
    },
    {
      "cell_type": "markdown",
      "id": "3ece5407",
      "metadata": {
        "id": "3ece5407"
      },
      "source": [
        "## **Modelagem**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2cf5a3a",
      "metadata": {
        "id": "e2cf5a3a"
      },
      "source": [
        "## **Carga**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c98c63fc",
      "metadata": {
        "id": "c98c63fc"
      },
      "source": [
        "## **An√°lise**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce8bfa5",
      "metadata": {
        "id": "5ce8bfa5"
      },
      "source": [
        "### **Qualidade dos dados**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34860ca",
      "metadata": {
        "id": "a34860ca"
      },
      "source": [
        "### **Solu√ß√£o do Problema**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Autoavalia√ß√£o**"
      ],
      "metadata": {
        "id": "qrTKgmttVgjA"
      },
      "id": "qrTKgmttVgjA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Desafios enfrentados**\n",
        "### **Etapa de Coleta e Carga para a Camada Bronze**\n"
      ],
      "metadata": {
        "id": "VBSEoiuPVjGA"
      },
      "id": "VBSEoiuPVjGA"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}